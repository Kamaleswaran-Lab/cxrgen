{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "933845dd-c87f-4c2c-8431-380c7678f6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "sys.path.append('/hpc/home/ma618/cxrgen/')\n",
    "sys.path.append('/hpc/home/ma618/cxrgen/src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f99c63e-b56f-42e3-8225-8ad4689be956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e3eb6fb-7fe7-42c6-8ff4-46f9059803de",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path('/hpc/group/kamaleswaranlab/EmoryDataset/Images/chest_xrays')\n",
    "dataset = root / 'longitudinal_data_corrected'\n",
    "embedding_path = dataset / 'image_embeddings'\n",
    "ehr_path = dataset / 'ehr_matrices'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e332556-8721-45df-aff4-af154ce81803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244895"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supertable_path = root / 'matched_supertables_with_images'\n",
    "sups = list(supertable_path.glob('*.pickle'))\n",
    "len(sups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "380c4cac-7293-44f3-bdb1-6b543cddda48",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temperature</th>\n",
       "      <th>daily_weight_kg</th>\n",
       "      <th>height_cm</th>\n",
       "      <th>sbp_line</th>\n",
       "      <th>dbp_line</th>\n",
       "      <th>map_line</th>\n",
       "      <th>sbp_cuff</th>\n",
       "      <th>dbp_cuff</th>\n",
       "      <th>map_cuff</th>\n",
       "      <th>pulse</th>\n",
       "      <th>...</th>\n",
       "      <th>Albumin 5%_dose</th>\n",
       "      <th>infection</th>\n",
       "      <th>sepsis</th>\n",
       "      <th>CXR_ACC_NUM_1</th>\n",
       "      <th>vent_mode</th>\n",
       "      <th>vent_rate_set</th>\n",
       "      <th>vent_tidal_rate_set</th>\n",
       "      <th>vent_tidal_rate_exhaled</th>\n",
       "      <th>peep</th>\n",
       "      <th>vent_fio2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-05-30 07:29:53</th>\n",
       "      <td>36.6</td>\n",
       "      <td>60.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AC/CMV Volume</td>\n",
       "      <td>12</td>\n",
       "      <td>450</td>\n",
       "      <td>402</td>\n",
       "      <td>5</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-30 08:29:53</th>\n",
       "      <td>36.6</td>\n",
       "      <td>54.1</td>\n",
       "      <td>188.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-30 09:29:53</th>\n",
       "      <td>36.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>107.333333</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>74.666667</td>\n",
       "      <td>61.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AC/CMV Volume</td>\n",
       "      <td>15</td>\n",
       "      <td>450</td>\n",
       "      <td>453</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-30 10:29:53</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109.333333</td>\n",
       "      <td>57.333333</td>\n",
       "      <td>79.666667</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-30 11:29:53</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>57.666667</td>\n",
       "      <td>79.666667</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-14 09:29:53</th>\n",
       "      <td>37.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>00004DX160001032</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-14 10:29:53</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>00004DX160001032</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-14 11:29:53</th>\n",
       "      <td>37.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>131.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>00004DX160001032</td>\n",
       "      <td>AC/CMV Volume</td>\n",
       "      <td>20</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-14 12:29:53</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>91.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>00004DX160001032</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-14 13:29:53</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>00004DX160001032</td>\n",
       "      <td>AC/CMV Volume</td>\n",
       "      <td>20</td>\n",
       "      <td>420</td>\n",
       "      <td>375</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>367 rows × 219 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     temperature  daily_weight_kg  height_cm  sbp_line  \\\n",
       "2016-05-30 07:29:53         36.6             60.0        NaN       NaN   \n",
       "2016-05-30 08:29:53         36.6             54.1      188.0       NaN   \n",
       "2016-05-30 09:29:53         36.0              NaN        NaN       NaN   \n",
       "2016-05-30 10:29:53          NaN              NaN        NaN       NaN   \n",
       "2016-05-30 11:29:53          NaN              NaN        NaN       NaN   \n",
       "...                          ...              ...        ...       ...   \n",
       "2016-06-14 09:29:53         37.7              NaN        NaN       NaN   \n",
       "2016-06-14 10:29:53          NaN              NaN        NaN       NaN   \n",
       "2016-06-14 11:29:53         37.8              NaN        NaN       NaN   \n",
       "2016-06-14 12:29:53          NaN              NaN        NaN       NaN   \n",
       "2016-06-14 13:29:53          NaN              NaN        NaN       NaN   \n",
       "\n",
       "                     dbp_line  map_line    sbp_cuff   dbp_cuff   map_cuff  \\\n",
       "2016-05-30 07:29:53       NaN       NaN   72.000000  36.000000        NaN   \n",
       "2016-05-30 08:29:53       NaN       NaN  134.000000  62.000000        NaN   \n",
       "2016-05-30 09:29:53       NaN       NaN  107.333333  55.000000  74.666667   \n",
       "2016-05-30 10:29:53       NaN       NaN  109.333333  57.333333  79.666667   \n",
       "2016-05-30 11:29:53       NaN       NaN  114.000000  57.666667  79.666667   \n",
       "...                       ...       ...         ...        ...        ...   \n",
       "2016-06-14 09:29:53       NaN       NaN  122.000000  58.000000  83.000000   \n",
       "2016-06-14 10:29:53       NaN       NaN         NaN        NaN        NaN   \n",
       "2016-06-14 11:29:53       NaN       NaN  131.000000  66.000000  92.000000   \n",
       "2016-06-14 12:29:53       NaN       NaN         NaN        NaN        NaN   \n",
       "2016-06-14 13:29:53       NaN       NaN         NaN        NaN        NaN   \n",
       "\n",
       "                         pulse  ...  Albumin 5%_dose  infection  sepsis  \\\n",
       "2016-05-30 07:29:53  75.000000  ...              NaN          0       0   \n",
       "2016-05-30 08:29:53  53.000000  ...              NaN          1       1   \n",
       "2016-05-30 09:29:53  61.666667  ...              NaN          1       1   \n",
       "2016-05-30 10:29:53  59.000000  ...              NaN          1       1   \n",
       "2016-05-30 11:29:53  65.000000  ...              NaN          1       1   \n",
       "...                        ...  ...              ...        ...     ...   \n",
       "2016-06-14 09:29:53  94.000000  ...              NaN          1       1   \n",
       "2016-06-14 10:29:53  88.000000  ...              NaN          1       1   \n",
       "2016-06-14 11:29:53  92.000000  ...              NaN          1       1   \n",
       "2016-06-14 12:29:53  91.500000  ...              NaN          1       1   \n",
       "2016-06-14 13:29:53        NaN  ...              NaN          1       1   \n",
       "\n",
       "                        CXR_ACC_NUM_1      vent_mode  vent_rate_set  \\\n",
       "2016-05-30 07:29:53               NaN  AC/CMV Volume             12   \n",
       "2016-05-30 08:29:53               NaN           None           None   \n",
       "2016-05-30 09:29:53               NaN  AC/CMV Volume             15   \n",
       "2016-05-30 10:29:53               NaN           None           None   \n",
       "2016-05-30 11:29:53               NaN           None           None   \n",
       "...                               ...            ...            ...   \n",
       "2016-06-14 09:29:53  00004DX160001032           None           None   \n",
       "2016-06-14 10:29:53  00004DX160001032           None           None   \n",
       "2016-06-14 11:29:53  00004DX160001032  AC/CMV Volume             20   \n",
       "2016-06-14 12:29:53  00004DX160001032           None           None   \n",
       "2016-06-14 13:29:53  00004DX160001032  AC/CMV Volume             20   \n",
       "\n",
       "                     vent_tidal_rate_set  vent_tidal_rate_exhaled  peep  \\\n",
       "2016-05-30 07:29:53                  450                      402     5   \n",
       "2016-05-30 08:29:53                 None                     None  None   \n",
       "2016-05-30 09:29:53                  450                      453     5   \n",
       "2016-05-30 10:29:53                 None                     None  None   \n",
       "2016-05-30 11:29:53                 None                     None  None   \n",
       "...                                  ...                      ...   ...   \n",
       "2016-06-14 09:29:53                 None                     None  None   \n",
       "2016-06-14 10:29:53                 None                     None  None   \n",
       "2016-06-14 11:29:53                 None                     None     5   \n",
       "2016-06-14 12:29:53                 None                     None  None   \n",
       "2016-06-14 13:29:53                  420                      375     5   \n",
       "\n",
       "                     vent_fio2  \n",
       "2016-05-30 07:29:53       0.50  \n",
       "2016-05-30 08:29:53       None  \n",
       "2016-05-30 09:29:53        0.4  \n",
       "2016-05-30 10:29:53       None  \n",
       "2016-05-30 11:29:53       None  \n",
       "...                        ...  \n",
       "2016-06-14 09:29:53       None  \n",
       "2016-06-14 10:29:53       None  \n",
       "2016-06-14 11:29:53        0.3  \n",
       "2016-06-14 12:29:53       None  \n",
       "2016-06-14 13:29:53        0.3  \n",
       "\n",
       "[367 rows x 219 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(sups[0])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66f2c174-3872-427e-a5c1-4e5fa2beefa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17690, 17690, 17690)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encounter_paths = list(ehr_path.glob(\"*.npy\"))\n",
    "prev_cxr_paths = list(embedding_path.glob(\"*_ffill_embeddings.npy\"))\n",
    "target_paths = list(embedding_path.glob(\"*_interpolated_embeddings.npy\"))\n",
    "len(encounter_paths), len(prev_cxr_paths), len(target_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c6c6775-bc49-41b8-aede-349ba4804b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "encounter_paths.sort()\n",
    "prev_cxr_paths.sort()\n",
    "target_paths.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c38c5971-13f0-4456-adc4-ce884a533244",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "model_type = 'transformer'\n",
    "max_seq_length = 100\n",
    "num_workers  = 8\n",
    "shuffle = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d935f754-9890-408b-a4e1-bbcbfc8963a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = dataloaders.create_encounter_dataloaders(encounter_paths=encounter_paths, \n",
    "                                        prev_cxr_paths=prev_cxr_paths,\n",
    "                                        target_paths=target_paths,\n",
    "                                        batch_size=batch_size,\n",
    "                                        model_type=model_type,\n",
    "                                        max_seq_length=max_seq_length,\n",
    "                                        num_workers=num_workers,\n",
    "                                        shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19dee438-4daf-4b64-bf3c-267676401277",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4d00d45-b5b3-4bf7-8967-4f463d40e2cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 100, 81]),\n",
       " torch.Size([8, 100, 512]),\n",
       " torch.Size([8, 100, 512]),\n",
       " torch.Size([8, 100]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['ehr'].shape, batch['prev_cxr'].shape, batch['target'].shape, batch['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5286e4e8-bb3d-4c3b-bda5-84ed1313c553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import src\n",
    "import src.models.transformer as transformer\n",
    "import src.models.transformernn as transformernn\n",
    "\n",
    "from tqdm import tqdm\n",
    "from src.training.trainer import TrainerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fad497a8-4523-4aca-97eb-3f481c51721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.configs.config_def as cf\n",
    "cf.set_paths()\n",
    "configs = cf.run_configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb67f05b-422e-4df8-be50-ec79f285a59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.models.transformernn' from '/hpc/home/ma618/cxrgen/src/models/transformernn.py'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(src.models.transformernn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97870b76-3e98-4219-a58f-3e0fc13f8599",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        'ehr_dim': 81,\n",
    "        'cxr_dim': 512,\n",
    "        'd_model': 512,\n",
    "        'num_encoder_layers': 6,\n",
    "        'num_decoder_layers': 6,\n",
    "        'num_heads': 8,\n",
    "        'mlp_ratio': 4.0,\n",
    "        'dropout': 0.1,\n",
    "        'max_seq_length': 100\n",
    "    }\n",
    "    \n",
    "# Create model\n",
    "model = transformernn.create_transformer_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8dae5b41-a4be-41be-916b-4a1f9a92fb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_mask = torch.isinf(batch['ehr'])\n",
    "inf_indices = torch.where(inf_mask)\n",
    "num_infs = inf_mask.sum().item()\n",
    "\n",
    "if inf_mask.any():\n",
    "    # For a multi-dimensional tensor\n",
    "    for idx in zip(*torch.where(inf_mask)):\n",
    "        print(f\"Inf found at index: {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75af69fc-5db2-4c5d-811a-2693f8e0de5a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EHR input\n",
      "tensor([[0.4167, 0.6767, 0.2889,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.6767, 0.2667,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.8000, 0.6767, 0.4667,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])\n",
      "ehr - min: -6.5, max: 531.0, has_nan: False\n",
      "EHR embedding\n",
      "tensor([[ 1.1481e-01,  9.9440e-02, -2.7472e-02,  ...,  4.8887e-01,\n",
      "          5.6493e-02, -2.2732e-01],\n",
      "        [ 1.2922e-01,  3.0371e-02,  9.9204e-05,  ...,  4.7830e-01,\n",
      "          7.8608e-02, -2.5675e-01],\n",
      "        [ 1.1892e-01,  1.2091e-01,  5.4149e-02,  ...,  4.7943e-01,\n",
      "          6.4979e-02, -3.3386e-01],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]], grad_fn=<SliceBackward0>)\n",
      "x - min: -39.074031829833984, max: 45.081153869628906, has_nan: False\n",
      "CXR condition\n",
      "tensor([[ 0.5270, -1.8931,  0.3415,  ...,  0.6366,  2.0353,  2.7105],\n",
      "        [ 0.5270, -1.8931,  0.3415,  ...,  0.6366,  2.0353,  2.7105],\n",
      "        [ 0.5270, -1.8931,  0.3415,  ...,  0.6366,  2.0353,  2.7105],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "cxr_cond - min: -5.148098468780518, max: 4.841726779937744, has_nan: False\n",
      "EHR embedding with CXR condition\n",
      "tensor([[ 0.6418, -1.7936,  0.3141,  ...,  1.1255,  2.0918,  2.4832],\n",
      "        [ 0.6562, -1.8627,  0.3416,  ...,  1.1149,  2.1139,  2.4538],\n",
      "        [ 0.6459, -1.7722,  0.3957,  ...,  1.1160,  2.1003,  2.3767],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "x - min: -39.222412109375, max: 41.93020248413086, has_nan: False\n",
      "EHR embedding with CXR condition and positional embedding\n",
      "tensor([[ 0.6418, -0.7936,  0.3141,  ...,  2.1255,  2.0918,  3.4832],\n",
      "        [ 1.4976, -1.3224,  1.1635,  ...,  2.1149,  2.1140,  3.4538],\n",
      "        [ 1.5552, -2.1883,  1.3321,  ...,  2.1160,  2.1005,  3.3767],\n",
      "        ...,\n",
      "        [ 0.3796, -0.9251, -0.6254,  ...,  0.9999,  0.0101,  0.9999],\n",
      "        [-0.5734, -0.8193,  0.2850,  ...,  0.9999,  0.0102,  0.9999],\n",
      "        [-0.9992,  0.0398,  0.9501,  ...,  0.9999,  0.0103,  0.9999]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "x - min: -38.584041595458984, max: 42.2247200012207, has_nan: False\n",
      "EHR embedding with CXR condition and positional embedding after dropout\n",
      "tensor([[ 0.7131, -0.8818,  0.3489,  ...,  2.3616,  2.3242,  3.8703],\n",
      "        [ 0.0000, -1.4693,  1.2928,  ...,  2.3499,  0.0000,  3.8376],\n",
      "        [ 1.7280, -2.4315,  1.4801,  ...,  2.3511,  2.3339,  3.7519],\n",
      "        ...,\n",
      "        [ 0.4218, -1.0279, -0.6949,  ...,  1.1111,  0.0112,  1.1111],\n",
      "        [-0.0000, -0.9103,  0.0000,  ...,  1.1110,  0.0113,  1.1111],\n",
      "        [-1.1102,  0.0442,  1.0557,  ...,  1.1110,  0.0114,  1.1111]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "x - min: -42.871158599853516, max: 46.837554931640625, has_nan: False\n",
      "key padding mask\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True])\n",
      "Attention mask\n",
      "None\n",
      "x_norm\n",
      "tensor([[ 0.0738, -0.7835, -0.1220,  ...,  0.9598,  0.9397,  1.7707],\n",
      "        [-0.3330, -1.1303,  0.3684,  ...,  0.9420, -0.3330,  1.7492],\n",
      "        [ 0.6234, -1.6430,  0.4883,  ...,  0.9629,  0.9535,  1.7262],\n",
      "        ...,\n",
      "        [ 0.1761, -1.9178, -1.4367,  ...,  1.1716, -0.4170,  1.1716],\n",
      "        [-0.4418, -1.7866, -0.4418,  ...,  1.1995, -0.4251,  1.1995],\n",
      "        [-2.0780, -0.3621,  1.1412,  ...,  1.2234, -0.4110,  1.2234]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "x_norm - min: -4.0751953125, max: 3.995443820953369, has_nan: False\n",
      "Attention output\n",
      "tensor([[-0.5008, -0.2262, -0.0796,  ...,  0.1339,  0.2218, -0.7486],\n",
      "        [-0.4914, -0.2529, -0.0791,  ...,  0.1359,  0.2064, -0.7465],\n",
      "        [-0.4498, -0.2330, -0.0593,  ...,  0.1259,  0.2323, -0.7198],\n",
      "        ...,\n",
      "        [-0.4582, -0.2480, -0.0827,  ...,  0.1333,  0.2087, -0.7481],\n",
      "        [-0.4635, -0.2392, -0.0716,  ...,  0.1290,  0.2195, -0.7249],\n",
      "        [-0.4381, -0.2306, -0.0680,  ...,  0.1246,  0.1917, -0.6887]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "key padding mask\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True])\n",
      "Attention mask\n",
      "None\n",
      "x_norm\n",
      "tensor([[-0.0604, -0.9891, -0.1987,  ...,  1.0140,  1.3066,  1.2831],\n",
      "        [-0.4329, -1.1600,  0.3254,  ...,  1.0236,  0.0093,  1.0164],\n",
      "        [ 0.5233, -1.7496,  0.4113,  ...,  1.2118,  1.2450,  1.0195],\n",
      "        ...,\n",
      "        [-0.2697, -2.7396, -1.3238,  ...,  1.5812,  0.1072,  0.1677],\n",
      "        [-0.9397, -2.1937,  0.1526,  ...,  1.9111, -0.0102,  0.0497],\n",
      "        [-2.3484, -1.4305,  0.9403,  ...,  1.6822,  0.4346, -0.1264]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "x_norm - min: -4.250248908996582, max: 3.994145393371582, has_nan: False\n",
      "Attention output\n",
      "tensor([[-0.2472,  0.1412, -0.0506,  ..., -0.2412, -0.2707, -0.1736],\n",
      "        [-0.2529,  0.1384, -0.0612,  ..., -0.2452, -0.2839, -0.1790],\n",
      "        [-0.2472,  0.1416, -0.0789,  ..., -0.2458, -0.2714, -0.1729],\n",
      "        ...,\n",
      "        [-0.2263,  0.1372, -0.0773,  ..., -0.2073, -0.2570, -0.1738],\n",
      "        [-0.2600,  0.1521, -0.0697,  ..., -0.2306, -0.2528, -0.1888],\n",
      "        [-0.2146,  0.1321, -0.0409,  ..., -0.2147, -0.2776, -0.1670]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "key padding mask\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True])\n",
      "Attention mask\n",
      "None\n",
      "x_norm\n",
      "tensor([[-0.2644, -0.8958, -0.3465,  ...,  0.9363,  0.9565,  1.1495],\n",
      "        [-0.6824, -1.0047,  0.1745,  ...,  0.9937, -0.4605,  0.7977],\n",
      "        [ 0.2153, -1.5836,  0.3147,  ...,  1.1115,  0.8405,  0.9206],\n",
      "        ...,\n",
      "        [-0.6569, -2.2135, -1.5626,  ...,  0.9425, -0.0304,  0.1768],\n",
      "        [-1.0930, -1.2451, -0.2016,  ...,  1.4813, -0.0492, -0.0153],\n",
      "        [-2.6378, -0.7053,  0.7910,  ...,  1.1552,  0.2035,  0.1202]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "x_norm - min: -4.228221416473389, max: 4.030966281890869, has_nan: False\n",
      "Attention output\n",
      "tensor([[-0.0833,  0.4049, -0.1918,  ...,  0.1552, -0.5327,  0.3689],\n",
      "        [-0.1005,  0.4420, -0.2167,  ...,  0.1409, -0.5379,  0.3898],\n",
      "        [-0.0723,  0.3954, -0.1539,  ...,  0.1273, -0.4942,  0.3630],\n",
      "        ...,\n",
      "        [-0.1184,  0.4282, -0.2345,  ...,  0.1450, -0.5457,  0.3983],\n",
      "        [-0.0947,  0.3652, -0.2398,  ...,  0.1406, -0.5108,  0.3656],\n",
      "        [-0.0730,  0.4192, -0.2345,  ...,  0.1496, -0.5370,  0.3848]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "key padding mask\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True])\n",
      "Attention mask\n",
      "None\n",
      "x_norm\n",
      "tensor([[-0.1732, -0.6537, -0.3081,  ...,  0.8734,  0.4906,  1.2380],\n",
      "        [-0.4206, -0.8082,  0.0884,  ...,  0.8785, -0.9801,  0.9537],\n",
      "        [ 0.3424, -1.5589,  0.2428,  ...,  1.0094,  0.4555,  0.9949],\n",
      "        ...,\n",
      "        [-0.3611, -1.5679, -0.9946,  ...,  1.0058, -0.6377, -0.4376],\n",
      "        [-0.7287, -0.4730,  0.4139,  ...,  1.2574, -0.7203, -0.1089],\n",
      "        [-2.4892, -0.1763,  0.9320,  ...,  1.1462, -0.2804, -0.1298]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "x_norm - min: -4.0869598388671875, max: 3.7515201568603516, has_nan: False\n",
      "Attention output\n",
      "tensor([[-0.1521, -0.1411, -0.0226,  ...,  0.0058, -0.3841,  0.0371],\n",
      "        [-0.1377, -0.1394, -0.0044,  ..., -0.0102, -0.3879,  0.0807],\n",
      "        [-0.1516, -0.1433, -0.0289,  ..., -0.0082, -0.4064,  0.0509],\n",
      "        ...,\n",
      "        [-0.1530, -0.1327, -0.0259,  ...,  0.0091, -0.4323,  0.0878],\n",
      "        [-0.1448, -0.1274, -0.0016,  ..., -0.0235, -0.3744,  0.0577],\n",
      "        [-0.1435, -0.1329, -0.0065,  ...,  0.0088, -0.4087,  0.0777]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "key padding mask\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True])\n",
      "Attention mask\n",
      "None\n",
      "x_norm\n",
      "tensor([[-0.1723, -0.7635, -0.3377,  ...,  0.8733,  0.1210,  1.1392],\n",
      "        [-0.4636, -0.8574,  0.0926,  ...,  0.8960, -1.1371,  0.9796],\n",
      "        [ 0.2348, -1.6424,  0.0656,  ...,  0.8471,  0.0600,  1.0013],\n",
      "        ...,\n",
      "        [-0.1533, -1.7204, -0.8150,  ...,  0.9816, -1.8356, -0.5015],\n",
      "        [-0.4094, -0.5595,  0.6215,  ...,  1.1688, -1.0782, -0.0839],\n",
      "        [-2.4221, -0.4046,  0.9353,  ...,  1.4406, -1.3850, -0.2602]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "x_norm - min: -4.107003688812256, max: 3.711594343185425, has_nan: False\n",
      "Attention output\n",
      "tensor([[-0.5497, -0.0757, -0.0290,  ..., -0.3387,  0.3481, -0.4442],\n",
      "        [-0.5764, -0.0862,  0.0026,  ..., -0.3552,  0.3350, -0.4507],\n",
      "        [-0.5724, -0.0877, -0.0103,  ..., -0.3709,  0.3418, -0.4793],\n",
      "        ...,\n",
      "        [-0.5696, -0.0727, -0.0324,  ..., -0.4210,  0.3016, -0.4852],\n",
      "        [-0.5582, -0.0862, -0.0210,  ..., -0.3928,  0.2810, -0.4768],\n",
      "        [-0.5645, -0.0816, -0.0175,  ..., -0.3663,  0.3312, -0.4533]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "key padding mask\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True])\n",
      "Attention mask\n",
      "None\n",
      "x_norm\n",
      "tensor([[-0.3391, -0.7763, -0.3435,  ...,  0.6630,  0.2359,  1.0017],\n",
      "        [-0.5649, -0.7811,  0.1183,  ...,  0.7799, -0.9225,  0.8346],\n",
      "        [ 0.0665, -1.5252, -0.0455,  ...,  0.7276,  0.3342,  0.8604],\n",
      "        ...,\n",
      "        [-0.4987, -1.6394, -0.5255,  ...,  0.1013, -1.6081, -0.9009],\n",
      "        [-0.8197, -0.5166,  0.5815,  ...,  0.3942, -0.9438, -0.1300],\n",
      "        [-2.6251, -0.4002,  0.9513,  ...,  0.6474, -0.9106, -0.4119]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "x_norm - min: -4.171638011932373, max: 3.976386547088623, has_nan: False\n",
      "Attention output\n",
      "tensor([[-0.3352, -0.2205, -0.2660,  ..., -0.0326,  0.2856,  0.2956],\n",
      "        [-0.2975, -0.2036, -0.2430,  ..., -0.0446,  0.2878,  0.2969],\n",
      "        [-0.3259, -0.2302, -0.2552,  ..., -0.0299,  0.3026,  0.2779],\n",
      "        ...,\n",
      "        [-0.3214, -0.2027, -0.2565,  ..., -0.0017,  0.2955,  0.2860],\n",
      "        [-0.3636, -0.2291, -0.2595,  ..., -0.0099,  0.3243,  0.2848],\n",
      "        [-0.3475, -0.2115, -0.2522,  ..., -0.0036,  0.2982,  0.3001]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Encoder output\n",
      "tensor([[-0.8543, -1.5938, -0.7620,  ...,  1.7412,  1.5068,  3.0362],\n",
      "        [-1.3625, -1.4843,  0.9833,  ...,  2.1651, -0.8935,  2.5573],\n",
      "        [ 0.2589, -2.9192,  0.2612,  ...,  1.7916,  1.6969,  2.5143],\n",
      "        ...,\n",
      "        [-0.8559, -1.9539, -0.4661,  ...,  0.3839, -1.1165, -0.4341],\n",
      "        [-0.9279, -0.6210,  0.7467,  ...,  0.6245, -0.5927,  0.1938],\n",
      "        [-2.6170, -0.6092,  1.1227,  ...,  0.9721, -0.4042, -0.0465]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Encoder output after norm\n",
      "tensor([[-0.6673, -1.0206, -0.6232,  ...,  0.5724,  0.4604,  1.1910],\n",
      "        [-0.9081, -0.9662,  0.2100,  ...,  0.7733, -0.6846,  0.9603],\n",
      "        [-0.1077, -1.6641, -0.1065,  ...,  0.6429,  0.5965,  0.9968],\n",
      "        ...,\n",
      "        [-0.9240, -1.8260, -0.6038,  ...,  0.0945, -1.1381, -0.5775],\n",
      "        [-1.0126, -0.7486,  0.4278,  ...,  0.3227, -0.7242, -0.0478],\n",
      "        [-2.4309, -0.7162,  0.7630,  ...,  0.6343, -0.5411, -0.2357]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Encoder output shape:\n",
      "tensor([[-0.6673, -1.0206, -0.6232,  ...,  0.5724,  0.4604,  1.1910],\n",
      "        [-0.9081, -0.9662,  0.2100,  ...,  0.7733, -0.6846,  0.9603],\n",
      "        [-0.1077, -1.6641, -0.1065,  ...,  0.6429,  0.5965,  0.9968],\n",
      "        ...,\n",
      "        [-0.9240, -1.8260, -0.6038,  ...,  0.0945, -1.1381, -0.5775],\n",
      "        [-1.0126, -0.7486,  0.4278,  ...,  0.3227, -0.7242, -0.0478],\n",
      "        [-2.4309, -0.7162,  0.7630,  ...,  0.6343, -0.5411, -0.2357]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder input\n",
      "tensor([[ 0.0298, -0.0223, -0.0172,  ...,  0.0159,  0.0158,  0.0434],\n",
      "        [ 0.0298, -0.0223, -0.0172,  ...,  0.0159,  0.0158,  0.0434],\n",
      "        [ 0.0298, -0.0223, -0.0172,  ...,  0.0159,  0.0158,  0.0434],\n",
      "        ...,\n",
      "        [ 0.0298, -0.0223, -0.0172,  ...,  0.0159,  0.0158,  0.0434],\n",
      "        [ 0.0298, -0.0223, -0.0172,  ...,  0.0159,  0.0158,  0.0434],\n",
      "        [ 0.0298, -0.0223, -0.0172,  ...,  0.0159,  0.0158,  0.0434]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder input after positional embeddings\n",
      "tensor([[ 0.0298,  0.9777, -0.0172,  ...,  1.0159,  0.0158,  1.0434],\n",
      "        [ 0.8712,  0.5180,  0.8046,  ...,  1.0159,  0.0160,  1.0434],\n",
      "        [ 0.9391, -0.4384,  0.9192,  ...,  1.0159,  0.0161,  1.0434],\n",
      "        ...,\n",
      "        [ 0.4094, -0.9474, -0.6426,  ...,  1.0158,  0.0259,  1.0433],\n",
      "        [-0.5436, -0.8416,  0.2678,  ...,  1.0158,  0.0260,  1.0433],\n",
      "        [-0.9694,  0.0175,  0.9329,  ...,  1.0158,  0.0261,  1.0433]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Causal mask\n",
      "tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "        [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
      "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., -inf, -inf],\n",
      "        [0., 0., 0.,  ..., 0., 0., -inf],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Decoder block input\n",
      "tensor([[-0.9400,  0.9461, -1.0335,  ...,  1.0221, -0.9677,  1.0767],\n",
      "        [ 0.7172, -0.0497,  0.5726,  ...,  1.0312, -1.1397,  1.0908],\n",
      "        [ 0.8645, -2.1408,  0.8211,  ...,  1.0321, -1.1493,  1.0920],\n",
      "        ...,\n",
      "        [ 0.1622, -1.9570, -1.4809,  ...,  1.1095, -0.4367,  1.1523],\n",
      "        [-1.3232, -1.7882, -0.0569,  ...,  1.1104, -0.4343,  1.1532],\n",
      "        [-1.9883, -0.4478,  0.9810,  ...,  1.1104, -0.4344,  1.1532]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block self-attention output\n",
      "tensor([[-0.4465, -0.2325, -0.2905,  ..., -0.1876, -0.1891,  0.1773],\n",
      "        [-0.3425, -0.2841, -0.3461,  ..., -0.0761, -0.1343,  0.2248],\n",
      "        [-0.2636, -0.1653, -0.2944,  ..., -0.1722, -0.2113,  0.0201],\n",
      "        ...,\n",
      "        [-0.4316, -0.1012, -0.4017,  ..., -0.1209, -0.3080, -0.1774],\n",
      "        [-0.4179, -0.0930, -0.4520,  ..., -0.1154, -0.3394, -0.1796],\n",
      "        [-0.3769, -0.0515, -0.4284,  ..., -0.1171, -0.3141, -0.1948]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block self-attention output\n",
      "tensor([[-0.4664,  0.7193, -0.3399,  ...,  0.8075, -0.1943,  1.2404],\n",
      "        [ 0.4907,  0.2024,  0.8046,  ...,  0.9313, -0.1332,  1.2931],\n",
      "        [ 0.6462, -0.4384,  0.5921,  ...,  0.8245, -0.2187,  1.0657],\n",
      "        ...,\n",
      "        [-0.0702, -1.0599, -1.0890,  ...,  0.8815,  0.0259,  0.8462],\n",
      "        [-1.0080, -0.9450, -0.2344,  ...,  0.8876, -0.3511,  0.8437],\n",
      "        [-1.3882, -0.0397,  0.4569,  ...,  0.8857, -0.3229,  0.8268]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block cross-attention input\n",
      "tensor([[-1.5545,  0.3125, -1.3554,  ...,  0.4512, -1.1261,  1.1328],\n",
      "        [-0.1197, -0.6093,  0.4134,  ...,  0.6286, -1.1792,  1.2429],\n",
      "        [ 0.1525, -1.7582,  0.0572,  ...,  0.4666, -1.3711,  0.8916],\n",
      "        ...,\n",
      "        [-0.5702, -2.0219, -2.0645,  ...,  0.8259, -0.4292,  0.7741],\n",
      "        [-1.9158, -1.8240, -0.7889,  ...,  0.8453, -0.9590,  0.7813],\n",
      "        [-2.4410, -0.5093,  0.2022,  ...,  0.8166, -0.9149,  0.7322]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block cross-attention output\n",
      "tensor([[-0.5156, -0.2447,  0.1565,  ...,  0.1259,  0.6812,  0.0925],\n",
      "        [-0.5025, -0.2328,  0.1678,  ...,  0.1289,  0.6680,  0.0929],\n",
      "        [-0.5165, -0.2443,  0.1340,  ...,  0.1201,  0.6721,  0.0735],\n",
      "        ...,\n",
      "        [-0.5133, -0.2567,  0.1521,  ...,  0.1632,  0.6245,  0.1205],\n",
      "        [-0.4978, -0.2309,  0.1452,  ...,  0.1492,  0.6415,  0.0989],\n",
      "        [-0.5139, -0.2631,  0.1600,  ...,  0.1786,  0.6403,  0.1190]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block cross-attention output\n",
      "tensor([[-1.0392,  0.4475, -0.1661,  ...,  0.9474, -0.1943,  1.3431],\n",
      "        [-0.0677, -0.0563,  0.9911,  ...,  1.0746,  0.6090,  1.3964],\n",
      "        [ 0.6462, -0.7099,  0.7410,  ...,  0.9579,  0.5281,  1.1474],\n",
      "        ...,\n",
      "        [-0.6405, -1.3451, -1.0890,  ...,  1.0628,  0.7198,  0.9801],\n",
      "        [-1.5611, -1.2015, -0.0730,  ...,  1.0534,  0.3617,  0.9536],\n",
      "        [-1.3882, -0.3321,  0.6346,  ...,  1.0842,  0.3885,  0.9590]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block MLP input\n",
      "tensor([[-2.1914, -0.1285, -0.9799,  ...,  0.5653, -1.0190,  1.1143],\n",
      "        [-0.9687, -0.9517,  0.6192,  ...,  0.7444,  0.0461,  1.2270],\n",
      "        [ 0.1110, -1.9384,  0.2543,  ...,  0.5821, -0.0675,  0.8684],\n",
      "        ...,\n",
      "        [-1.2993, -2.2404, -1.8982,  ...,  0.9757,  0.5176,  0.8652],\n",
      "        [-2.5264, -2.0452, -0.5350,  ...,  0.9724,  0.0467,  0.8389],\n",
      "        [-2.2647, -0.8718,  0.4033,  ...,  0.9961,  0.0786,  0.8311]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block MLP output\n",
      "tensor([[-1.0216, -0.0248, -0.1661,  ...,  1.2146, -0.6112,  1.3336],\n",
      "        [-0.0133, -0.3368,  1.2136,  ...,  1.3627,  0.3605,  1.5451],\n",
      "        [ 0.5857, -0.7099,  0.9492,  ...,  1.1544,  0.3101,  1.1474],\n",
      "        ...,\n",
      "        [-0.5861, -1.5891, -0.7383,  ...,  1.4458,  0.8370,  1.2386],\n",
      "        [-1.4414, -1.0990,  0.3087,  ...,  1.5262,  0.3617,  1.1182],\n",
      "        [-1.3882, -0.3690,  0.8016,  ...,  1.9147,  0.6574,  1.0838]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block input\n",
      "tensor([[-1.9850, -0.7170, -0.8968,  ...,  0.8595, -1.4630,  1.0108],\n",
      "        [-0.8410, -1.2939,  0.8760,  ...,  1.0846, -0.3180,  1.3398],\n",
      "        [ 0.0312, -1.7769,  0.5384,  ...,  0.8248, -0.3534,  0.8150],\n",
      "        ...,\n",
      "        [-1.1545, -2.4231, -1.3471,  ...,  1.4153,  0.6453,  1.1533],\n",
      "        [-2.2250, -1.7952, -0.0281,  ...,  1.5002,  0.0384,  0.9880],\n",
      "        [-2.1211, -0.8625,  0.5831,  ...,  1.9577,  0.4050,  0.9317]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block self-attention output\n",
      "tensor([[ 0.0050, -0.3459, -0.2156,  ...,  0.0536,  0.1887,  0.2187],\n",
      "        [-0.2662, -0.2096, -0.3164,  ...,  0.1965, -0.1001,  0.1766],\n",
      "        [-0.4034, -0.2438, -0.1281,  ...,  0.0933, -0.0838,  0.2178],\n",
      "        ...,\n",
      "        [-0.0991, -0.3116, -0.0937,  ..., -0.2004,  0.0611, -0.4399],\n",
      "        [-0.1054, -0.3012, -0.0763,  ..., -0.2015,  0.0905, -0.4321],\n",
      "        [-0.1087, -0.3023, -0.0409,  ..., -0.2036,  0.0802, -0.3799]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block self-attention output\n",
      "tensor([[-1.0160, -0.4091, -0.4056,  ...,  1.2146, -0.6112,  1.3336],\n",
      "        [-0.3090, -0.5697,  1.2136,  ...,  1.5811,  0.2492,  1.7413],\n",
      "        [ 0.1375, -0.9808,  0.8068,  ...,  1.2581,  0.2171,  1.1474],\n",
      "        ...,\n",
      "        [-0.6961, -1.9353, -0.8425,  ...,  1.2230,  0.9048,  0.7499],\n",
      "        [-1.5585, -1.0990,  0.2240,  ...,  1.3022,  0.4622,  0.6381],\n",
      "        [-1.5089, -0.7049,  0.7562,  ...,  1.6885,  0.7465,  0.6618]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block cross-attention input\n",
      "tensor([[-1.8109, -1.1006, -1.0966,  ...,  0.7997, -1.3372,  0.9388],\n",
      "        [-1.1071, -1.4294,  0.7753,  ...,  1.2295, -0.4170,  1.4276],\n",
      "        [-0.5278, -1.9075,  0.2980,  ...,  0.8547, -0.4296,  0.7181],\n",
      "        ...,\n",
      "        [-1.2257, -2.6991, -1.3997,  ...,  1.0561,  0.6778,  0.4935],\n",
      "        [-2.2378, -1.6965, -0.1381,  ...,  1.1319,  0.1425,  0.3496],\n",
      "        [-2.1595, -1.2233,  0.4776,  ...,  1.5630,  0.4663,  0.3677]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block cross-attention output\n",
      "tensor([[-0.2972, -0.3515, -0.1280,  ..., -0.0939, -0.1523,  0.0082],\n",
      "        [-0.3147, -0.3334, -0.1271,  ..., -0.0883, -0.1551,  0.0242],\n",
      "        [-0.2916, -0.3219, -0.1246,  ..., -0.0802, -0.1770,  0.0073],\n",
      "        ...,\n",
      "        [-0.3113, -0.3224, -0.1303,  ..., -0.0926, -0.1127,  0.0414],\n",
      "        [-0.3092, -0.2899, -0.1572,  ..., -0.1079, -0.1442,  0.0549],\n",
      "        [-0.3202, -0.2980, -0.1247,  ..., -0.1105, -0.1314,  0.0426]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block cross-attention output\n",
      "tensor([[-1.3462, -0.7996, -0.5479,  ...,  1.1103, -0.7804,  1.3426],\n",
      "        [-0.6587, -0.9402,  1.0724,  ...,  1.4830,  0.0769,  1.7682],\n",
      "        [-0.1865, -1.3385,  0.6683,  ...,  1.1689,  0.0204,  1.1555],\n",
      "        ...,\n",
      "        [-1.0420, -2.2936, -0.8425,  ...,  1.2230,  0.7796,  0.7958],\n",
      "        [-1.9022, -1.4211,  0.0494,  ...,  1.1823,  0.3020,  0.6990],\n",
      "        [-1.8647, -1.0359,  0.6176,  ...,  1.5657,  0.6005,  0.7091]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block MLP input\n",
      "tensor([[-2.0285, -1.4336, -1.1596,  ...,  0.6453, -1.4127,  0.8981],\n",
      "        [-1.3796, -1.6951,  0.5600,  ...,  1.0201, -0.5554,  1.3397],\n",
      "        [-0.8261, -2.1053,  0.1232,  ...,  0.6790, -0.5964,  0.6641],\n",
      "        ...,\n",
      "        [-1.4890, -2.8471, -1.2725,  ...,  0.9689,  0.4877,  0.5053],\n",
      "        [-2.4154, -1.8964, -0.3100,  ...,  0.9123, -0.0375,  0.3909],\n",
      "        [-2.3678, -1.4810,  0.2884,  ...,  1.3030,  0.2701,  0.3863]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block MLP output\n",
      "tensor([[-1.3029, -0.7853, -0.5568,  ...,  1.1036, -0.6144,  0.9093],\n",
      "        [-0.6301, -1.0845,  0.9852,  ...,  1.3474,  0.1681,  1.3754],\n",
      "        [-0.1497, -1.3989,  0.5769,  ...,  1.1118,  0.1624,  0.7042],\n",
      "        ...,\n",
      "        [-1.2542, -2.4173, -0.6896,  ...,  1.2307,  1.1149,  0.5659],\n",
      "        [-1.9708, -1.4576,  0.3737,  ...,  1.1823,  0.7828,  0.5606],\n",
      "        [-1.5888, -1.0518,  0.8142,  ...,  1.6955,  0.9402,  0.4455]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block input\n",
      "tensor([[-1.9315, -1.3835, -1.1415,  ...,  0.6163, -1.2026,  0.4106],\n",
      "        [-1.3225, -1.8170,  0.4356,  ...,  0.8298, -0.4538,  0.8603],\n",
      "        [-0.7700, -2.1110,  0.0101,  ...,  0.5842, -0.4349,  0.1468],\n",
      "        ...,\n",
      "        [-1.6585, -2.8737, -1.0687,  ...,  0.9375,  0.8164,  0.2429],\n",
      "        [-2.4597, -1.9143,  0.0319,  ...,  0.8913,  0.4666,  0.2305],\n",
      "        [-2.0295, -1.4670,  0.4877,  ...,  1.4109,  0.6197,  0.1015]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block self-attention output\n",
      "tensor([[ 0.1345, -0.2648, -0.0572,  ...,  0.0259,  0.8247, -0.0648],\n",
      "        [ 0.0570, -0.2546, -0.0338,  ..., -0.0367,  0.6641, -0.0583],\n",
      "        [-0.1122, -0.1164,  0.0566,  ...,  0.0430,  0.5296,  0.0280],\n",
      "        ...,\n",
      "        [ 0.0904, -0.2645,  0.1768,  ..., -0.0744,  0.3808,  0.0523],\n",
      "        [ 0.0773, -0.2645,  0.1772,  ..., -0.0527,  0.4008,  0.0432],\n",
      "        [ 0.0599, -0.2535,  0.1704,  ..., -0.0375,  0.3766,  0.0427]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block self-attention output\n",
      "tensor([[-1.1534, -1.0796, -0.6204,  ...,  1.1324,  0.3019,  0.8374],\n",
      "        [-0.5667, -1.3673,  0.9852,  ...,  1.3066,  0.9060,  1.3106],\n",
      "        [-0.1497, -1.5283,  0.6398,  ...,  1.1596,  0.7509,  0.7042],\n",
      "        ...,\n",
      "        [-1.1537, -2.7111, -0.6896,  ...,  1.1481,  1.5380,  0.6239],\n",
      "        [-1.8849, -1.7515,  0.5706,  ...,  1.1238,  0.7828,  0.6086],\n",
      "        [-1.5223, -1.3335,  1.0035,  ...,  1.6538,  1.3586,  0.4929]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block cross-attention input\n",
      "tensor([[-1.6120, -1.5403, -1.0947,  ...,  0.6064, -0.1996,  0.3201],\n",
      "        [-1.1590, -1.9658,  0.4051,  ...,  0.7289,  0.3252,  0.7330],\n",
      "        [-0.7201, -2.1138,  0.0781,  ...,  0.6036,  0.1904,  0.1433],\n",
      "        ...,\n",
      "        [-1.4822, -3.0371, -1.0188,  ...,  0.8159,  1.2052,  0.2926],\n",
      "        [-2.2498, -2.1148,  0.2352,  ...,  0.7950,  0.4499,  0.2737],\n",
      "        [-1.8918, -1.7009,  0.6621,  ...,  1.3196,  1.0212,  0.1458]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block cross-attention output\n",
      "tensor([[-0.2436, -0.0297,  0.1760,  ...,  0.4749,  0.2186,  0.3401],\n",
      "        [-0.2453, -0.0087,  0.1781,  ...,  0.4510,  0.2126,  0.3380],\n",
      "        [-0.1732, -0.0244,  0.1705,  ...,  0.4604,  0.2035,  0.3727],\n",
      "        ...,\n",
      "        [-0.2309, -0.0152,  0.1649,  ...,  0.4342,  0.2152,  0.3454],\n",
      "        [-0.2346, -0.0270,  0.1792,  ...,  0.4459,  0.2135,  0.3551],\n",
      "        [-0.2267, -0.0250,  0.1704,  ...,  0.4816,  0.1900,  0.3883]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block cross-attention output\n",
      "tensor([[-1.4242, -1.1126, -0.4248,  ...,  1.6601,  0.5449,  1.2153],\n",
      "        [-0.8393, -1.3770,  1.1831,  ...,  1.8077,  1.1423,  1.6861],\n",
      "        [-0.3422, -1.5283,  0.8293,  ...,  1.6711,  0.9770,  1.1184],\n",
      "        ...,\n",
      "        [-1.4103, -2.7280, -0.5064,  ...,  1.6305,  1.7771,  1.0077],\n",
      "        [-2.1456, -1.7815,  0.7696,  ...,  1.6193,  1.0200,  0.6086],\n",
      "        [-1.7741, -1.3613,  1.1929,  ...,  2.1889,  1.5698,  0.9243]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block MLP input\n",
      "tensor([[-1.7924, -1.5033, -0.8650,  ...,  1.0699,  0.0349,  0.6571],\n",
      "        [-1.3731, -1.8937,  0.5847,  ...,  1.1894,  0.5452,  1.0717],\n",
      "        [-0.8804, -2.0359,  0.2610,  ...,  1.0811,  0.4049,  0.5426],\n",
      "        ...,\n",
      "        [-1.6735, -2.9409, -0.8042,  ...,  1.2511,  1.3921,  0.6522],\n",
      "        [-2.4337, -2.0768,  0.4241,  ...,  1.2569,  0.6695,  0.2662],\n",
      "        [-2.0848, -1.6780,  0.8393,  ...,  1.8210,  1.2109,  0.5747]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block MLP output\n",
      "tensor([[-0.9260, -1.2035, -0.3168,  ...,  2.0726,  0.5449,  1.3849],\n",
      "        [-0.5789, -1.3515,  1.1310,  ...,  1.9711,  1.6291,  1.7177],\n",
      "        [-0.3422, -1.5883,  0.8851,  ...,  1.9926,  0.9770,  1.3116],\n",
      "        ...,\n",
      "        [-1.0024, -2.6620, -0.1928,  ...,  1.9329,  2.4595,  1.0077],\n",
      "        [-1.7388, -1.9778,  1.1185,  ...,  1.9874,  1.8338,  0.5953],\n",
      "        [-1.3981, -1.3996,  1.2761,  ...,  2.4448,  2.2174,  1.2266]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block input\n",
      "tensor([[-1.2969, -1.5499, -0.7416,  ...,  1.4370,  0.0441,  0.8100],\n",
      "        [-1.0866, -1.8157,  0.5270,  ...,  1.3197,  0.9970,  1.0807],\n",
      "        [-0.8520, -2.0366,  0.3146,  ...,  1.3673,  0.4020,  0.7200],\n",
      "        ...,\n",
      "        [-1.2396, -2.7664, -0.4949,  ...,  1.4608,  1.9451,  0.6096],\n",
      "        [-1.9741, -2.1989,  0.7135,  ...,  1.5308,  1.3864,  0.2214],\n",
      "        [-1.6773, -1.6788,  0.8834,  ...,  2.0025,  1.7848,  0.8360]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block self-attention output\n",
      "tensor([[ 0.4738, -1.1201,  0.2733,  ..., -0.9930, -0.0836, -0.3254],\n",
      "        [ 0.3513, -0.9629,  0.3585,  ..., -0.9153, -0.2323, -0.4408],\n",
      "        [ 0.2925, -0.9261,  0.3006,  ..., -0.8974, -0.2801, -0.3936],\n",
      "        ...,\n",
      "        [ 0.4924, -0.6104,  0.3818,  ..., -1.0094, -0.3057, -0.3232],\n",
      "        [ 0.4421, -0.5666,  0.4098,  ..., -0.9969, -0.3051, -0.3147],\n",
      "        [ 0.4611, -0.5982,  0.3889,  ..., -0.9939, -0.2948, -0.3257]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block self-attention output\n",
      "tensor([[-0.3995, -2.4481, -0.0132,  ...,  0.9693,  0.4520,  1.0232],\n",
      "        [-0.1886, -2.4213,  1.5293,  ...,  1.9711,  1.3710,  1.2280],\n",
      "        [-0.3422, -2.6174,  1.2190,  ...,  0.9954,  0.6657,  0.8743],\n",
      "        ...,\n",
      "        [-0.4553, -3.3403,  0.2314,  ...,  0.8113,  2.1197,  0.6486],\n",
      "        [-1.7388, -2.6073,  1.5738,  ...,  0.8797,  1.4949,  0.2457],\n",
      "        [-0.8858, -2.0643,  1.7081,  ...,  1.3404,  1.8899,  0.8646]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block cross-attention input\n",
      "tensor([[-0.7602, -2.5139, -0.4295,  ...,  0.4115, -0.0313,  0.4578],\n",
      "        [-0.6615, -2.6485,  0.8673,  ...,  1.2604,  0.7264,  0.5992],\n",
      "        [-0.7820, -2.8147,  0.6128,  ...,  0.4130,  0.1185,  0.3048],\n",
      "        ...,\n",
      "        [-0.6882, -3.2372, -0.0814,  ...,  0.4310,  1.5871,  0.2872],\n",
      "        [-1.8627, -2.6413,  1.1069,  ...,  0.4847,  1.0362, -0.0837],\n",
      "        [-1.1017, -2.1729,  1.2558,  ...,  0.9215,  1.4209,  0.4891]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block cross-attention output\n",
      "tensor([[ 0.1131, -0.0299, -0.2626,  ..., -0.1401,  0.1888,  0.0394],\n",
      "        [ 0.1214, -0.0344, -0.2862,  ..., -0.1688,  0.1922,  0.0622],\n",
      "        [ 0.1175, -0.0137, -0.2486,  ..., -0.1476,  0.1897,  0.0542],\n",
      "        ...,\n",
      "        [ 0.1404, -0.0373, -0.2481,  ..., -0.1408,  0.1693,  0.0404],\n",
      "        [ 0.1284,  0.0047, -0.2433,  ..., -0.1576,  0.2194,  0.0328],\n",
      "        [ 0.0993, -0.0292, -0.2569,  ..., -0.1548,  0.2014,  0.0534]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block cross-attention output\n",
      "tensor([[-0.2738, -2.4812, -0.3050,  ...,  0.8136,  0.6618,  1.0670],\n",
      "        [-0.0537, -2.4596,  1.5293,  ...,  1.7835,  1.5846,  1.2971],\n",
      "        [-0.2116, -2.6326,  0.9428,  ...,  0.9954,  0.8765,  0.9345],\n",
      "        ...,\n",
      "        [-0.2993, -3.3403, -0.0442,  ...,  0.6549,  2.3078,  0.6935],\n",
      "        [-1.5962, -2.6021,  1.3034,  ...,  0.7047,  1.7387,  0.2821],\n",
      "        [-0.7755, -2.0967,  1.4227,  ...,  1.1684,  2.1136,  0.9239]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block MLP input\n",
      "tensor([[-0.6359, -2.4765, -0.6619,  ...,  0.2708,  0.1442,  0.4821],\n",
      "        [-0.5272, -2.6156,  0.8469,  ...,  1.0676,  0.8949,  0.6454],\n",
      "        [-0.6537, -2.7579,  0.3496,  ...,  0.3953,  0.2919,  0.3424],\n",
      "        ...,\n",
      "        [-0.5401, -3.1352, -0.3224,  ...,  0.2743,  1.6849,  0.3072],\n",
      "        [-1.6934, -2.5719,  0.8388,  ...,  0.3159,  1.2189, -0.0531],\n",
      "        [-0.9747, -2.1343,  0.9546,  ...,  0.7313,  1.5610,  0.5168]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block MLP output\n",
      "tensor([[-0.4216, -2.8163, -0.6426,  ...,  0.4988,  0.4646,  1.0670],\n",
      "        [-0.3717, -2.6661,  1.1814,  ...,  1.3801,  1.2152,  1.8428],\n",
      "        [-0.2116, -2.7207,  0.7775,  ...,  0.6873,  0.6901,  1.3177],\n",
      "        ...,\n",
      "        [-0.3061, -3.7958, -0.3962,  ...,  0.2043,  2.3548,  1.3114],\n",
      "        [-1.6407, -2.7397,  0.8803,  ...,  0.1673,  1.6876,  0.2821],\n",
      "        [-0.7394, -2.3466,  1.1767,  ...,  0.7933,  1.9848,  1.4678]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block input\n",
      "tensor([[-0.7376, -2.6669, -0.9156,  ...,  0.0039, -0.0236,  0.4617],\n",
      "        [-0.7818, -2.6858,  0.5070,  ...,  0.6719,  0.5350,  1.0559],\n",
      "        [-0.6244, -2.6902,  0.1900,  ...,  0.1157,  0.1180,  0.6347],\n",
      "        ...,\n",
      "        [-0.5258, -3.4104, -0.6003,  ..., -0.1039,  1.6737,  0.8112],\n",
      "        [-1.6570, -2.5813,  0.4630,  ..., -0.1366,  1.1420, -0.0400],\n",
      "        [-0.9119, -2.2772,  0.7158,  ...,  0.3901,  1.4023,  0.9631]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block self-attention output\n",
      "tensor([[-0.0165, -0.6607, -1.0388,  ..., -0.1920, -0.0482, -0.2291],\n",
      "        [-0.1305, -0.4618, -0.9434,  ..., -0.5486,  0.1280, -0.0645],\n",
      "        [-0.1574, -0.2276, -0.8020,  ..., -0.5667, -0.0138, -0.1706],\n",
      "        ...,\n",
      "        [-0.1611, -0.0133, -0.4647,  ..., -0.6210,  0.1332,  0.1302],\n",
      "        [-0.1553, -0.0392, -0.5167,  ..., -0.6398,  0.1504,  0.1157],\n",
      "        [-0.1494, -0.0241, -0.4996,  ..., -0.6424,  0.1624,  0.1253]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block self-attention output\n",
      "tensor([[-0.4399, -3.5504, -1.7968,  ...,  0.2854,  0.4110,  0.8124],\n",
      "        [-0.5166, -3.1791,  0.1332,  ...,  0.7705,  1.3573,  1.7711],\n",
      "        [-0.3865, -2.9736, -0.1136,  ...,  0.0577,  0.6748,  1.1282],\n",
      "        ...,\n",
      "        [-0.4851, -3.8105, -0.9125,  ..., -0.4857,  2.5028,  1.4561],\n",
      "        [-1.8132, -2.7833,  0.3062,  ..., -0.5436,  1.6876,  0.4106],\n",
      "        [-0.9054, -2.3735,  0.6216,  ...,  0.0795,  2.1652,  1.6070]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block cross-attention input\n",
      "tensor([[-0.7079, -3.1056, -1.7538,  ..., -0.1487, -0.0518,  0.2576],\n",
      "        [-0.8566, -2.9523, -0.3451,  ...,  0.1566,  0.6185,  0.9442],\n",
      "        [-0.7569, -2.8156, -0.5397,  ..., -0.4034,  0.0877,  0.4484],\n",
      "        ...,\n",
      "        [-0.6684, -3.3313, -1.0107,  ..., -0.6688,  1.7243,  0.8861],\n",
      "        [-1.7375, -2.5195, -0.0289,  ..., -0.7140,  1.0849,  0.0553],\n",
      "        [-1.0291, -2.2275,  0.2174,  ..., -0.2251,  1.4775,  1.0218]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block cross-attention output\n",
      "tensor([[-0.1892, -0.1280,  0.3719,  ..., -0.1542, -0.0764, -0.0270],\n",
      "        [-0.2089, -0.1006,  0.3367,  ..., -0.1666, -0.0904, -0.0497],\n",
      "        [-0.2073, -0.1163,  0.2990,  ..., -0.1885, -0.0743, -0.0340],\n",
      "        ...,\n",
      "        [-0.2000, -0.0938,  0.3189,  ..., -0.1632, -0.1025, -0.0332],\n",
      "        [-0.1768, -0.0809,  0.2983,  ..., -0.1864, -0.0680, -0.0400],\n",
      "        [-0.1781, -0.1000,  0.3069,  ..., -0.1506, -0.1068, -0.0170]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block cross-attention output\n",
      "tensor([[-0.6501, -3.6925, -1.3836,  ...,  0.1141,  0.4110,  0.7824],\n",
      "        [-0.7488, -3.2909,  0.5073,  ...,  0.5855,  1.2569,  1.7159],\n",
      "        [-0.6168, -3.1029,  0.2186,  ..., -0.1517,  0.5922,  1.0904],\n",
      "        ...,\n",
      "        [-0.7074, -3.9147, -0.5582,  ..., -0.6670,  2.3890,  1.4191],\n",
      "        [-2.0097, -2.8732,  0.6376,  ..., -0.7507,  1.6121,  0.3661],\n",
      "        [-1.1033, -2.4846,  0.9626,  ..., -0.0878,  2.0466,  1.5881]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block MLP input\n",
      "tensor([[-0.8371, -3.0924, -1.3808,  ..., -0.2707, -0.0505,  0.2247],\n",
      "        [-1.0041, -2.9249, -0.0550,  ...,  0.0041,  0.5115,  0.8583],\n",
      "        [-0.9073, -2.8082, -0.2685,  ..., -0.5517,  0.0172,  0.3981],\n",
      "        ...,\n",
      "        [-0.8124, -3.2619, -0.6985,  ..., -0.7816,  1.5522,  0.8115],\n",
      "        [-1.8139, -2.4776,  0.2211,  ..., -0.8461,  0.9702,  0.0124],\n",
      "        [-1.1430, -2.2241,  0.4740,  ..., -0.3482,  1.3225,  0.9636]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block MLP output\n",
      "tensor([[-0.9432, -3.6020, -1.1729,  ...,  0.3459,  0.8966,  1.1661],\n",
      "        [-0.7146, -3.3933,  0.6988,  ...,  0.5585,  1.4939,  2.2304],\n",
      "        [-0.7479, -2.9328,  0.2113,  ..., -0.1976,  0.5922,  1.3036],\n",
      "        ...,\n",
      "        [-0.6586, -3.9312, -0.2431,  ..., -0.5795,  2.8971,  1.4553],\n",
      "        [-1.5865, -2.8917,  0.8927,  ..., -0.6667,  1.9830,  0.4428],\n",
      "        [-0.9130, -2.4609,  1.3896,  ...,  0.1232,  2.3998,  1.8375]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block input\n",
      "tensor([[-1.0446, -2.9923, -1.2128,  ..., -0.1003,  0.3032,  0.5006],\n",
      "        [-0.9678, -2.9623,  0.0846,  ..., -0.0198,  0.6766,  1.2250],\n",
      "        [-1.0011, -2.6535, -0.2756,  ..., -0.5849,  0.0124,  0.5505],\n",
      "        ...,\n",
      "        [-0.7641, -3.2003, -0.4549,  ..., -0.7053,  1.8827,  0.8095],\n",
      "        [-1.4535, -2.4294,  0.4000,  ..., -0.7659,  1.2153,  0.0637],\n",
      "        [-0.9820, -2.1722,  0.7886,  ..., -0.1852,  1.5654,  1.1330]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block self-attention output\n",
      "tensor([[-0.1680, -0.0176, -0.1872,  ...,  0.3941, -0.3425, -0.4775],\n",
      "        [-0.2090,  0.1431, -0.1364,  ...,  0.3280, -0.3564, -0.3762],\n",
      "        [-0.1007,  0.0914, -0.1761,  ...,  0.4462, -0.2688, -0.3810],\n",
      "        ...,\n",
      "        [ 0.0441, -0.0517,  0.1251,  ...,  0.4962, -0.1659, -0.3828],\n",
      "        [ 0.0155, -0.0656,  0.1143,  ...,  0.5213, -0.1608, -0.3811],\n",
      "        [ 0.0093, -0.0672,  0.1600,  ...,  0.5553, -0.1676, -0.4080]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block self-attention output\n",
      "tensor([[-1.1299, -3.6216, -1.3809,  ...,  0.7837,  0.5161,  0.6356],\n",
      "        [-0.9468, -3.2343,  0.5473,  ...,  0.9230,  1.0979,  1.8124],\n",
      "        [-0.8598, -2.8312,  0.0157,  ...,  0.2981,  0.2936,  0.8803],\n",
      "        ...,\n",
      "        [-0.6095, -3.9886, -0.1041,  ..., -0.0282,  2.7128,  1.0300],\n",
      "        [-1.5693, -2.9646,  1.0196,  ..., -0.0875,  1.8044,  0.4428],\n",
      "        [-0.9027, -2.5357,  1.5674,  ...,  0.1232,  2.2136,  1.3842]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block cross-attention input\n",
      "tensor([[-1.1563, -2.9269, -1.3346,  ...,  0.2036,  0.0134,  0.0983],\n",
      "        [-1.1290, -2.7967, -0.0398,  ...,  0.2341,  0.3616,  0.8825],\n",
      "        [-1.0685, -2.5175, -0.4250,  ..., -0.2174, -0.2207,  0.2106],\n",
      "        ...,\n",
      "        [-0.7174, -3.1635, -0.3515,  ..., -0.2965,  1.6876,  0.4694],\n",
      "        [-1.4130, -2.4280,  0.4702,  ..., -0.3351,  1.0411,  0.0506],\n",
      "        [-0.9572, -2.1800,  0.8925,  ..., -0.1890,  1.3764,  0.7553]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block cross-attention output\n",
      "tensor([[-0.0176,  0.0096, -0.0537,  ..., -0.0675, -0.0379,  0.1022],\n",
      "        [ 0.0143,  0.0812, -0.0492,  ..., -0.0899, -0.0251,  0.0829],\n",
      "        [ 0.0088,  0.0733, -0.0843,  ..., -0.0802, -0.0275,  0.1223],\n",
      "        ...,\n",
      "        [ 0.0055,  0.0709, -0.0421,  ..., -0.0890, -0.0223,  0.0804],\n",
      "        [-0.0022,  0.0786, -0.0532,  ..., -0.0894, -0.0299,  0.0948],\n",
      "        [-0.0122,  0.0412, -0.0554,  ..., -0.0805, -0.0259,  0.1063]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block cross-attention output\n",
      "tensor([[-1.1495, -3.6109, -1.3809,  ...,  0.7087,  0.4739,  0.7491],\n",
      "        [-0.9468, -3.1441,  0.4926,  ...,  0.8231,  1.0700,  1.8124],\n",
      "        [-0.8500, -2.7498, -0.0780,  ...,  0.2981,  0.2630,  1.0162],\n",
      "        ...,\n",
      "        [-0.6034, -3.9098, -0.1508,  ..., -0.1270,  2.6880,  1.1194],\n",
      "        [-1.5717, -2.8773,  0.9605,  ..., -0.1868,  1.7712,  0.4428],\n",
      "        [-0.9162, -2.4899,  1.5674,  ...,  0.0337,  2.2136,  1.5023]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block MLP input\n",
      "tensor([[-1.1581, -2.8496, -1.3171,  ...,  0.1189, -0.0425,  0.1466],\n",
      "        [-1.1073, -2.6377, -0.1047,  ...,  0.1255,  0.2974,  0.8145],\n",
      "        [-1.0496, -2.3882, -0.5055,  ..., -0.2405, -0.2653,  0.2655],\n",
      "        ...,\n",
      "        [-0.7140, -3.0151, -0.3990,  ..., -0.3824,  1.5767,  0.4850],\n",
      "        [-1.3916, -2.3077,  0.3851,  ..., -0.4200,  0.9538,  0.0218],\n",
      "        [-0.9549, -2.0814,  0.8229,  ..., -0.2750,  1.2854,  0.7762]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block MLP output\n",
      "tensor([[-1.1326e+00, -3.8805e+00, -1.5722e+00,  ...,  6.0635e-01,\n",
      "          3.9236e-01,  1.0604e+00],\n",
      "        [-9.3922e-01, -3.4363e+00,  1.4370e-01,  ...,  7.2104e-01,\n",
      "          1.0700e+00,  2.0958e+00],\n",
      "        [-7.6156e-01, -3.2821e+00, -3.2621e-01,  ...,  2.9814e-01,\n",
      "          6.2650e-02,  1.3434e+00],\n",
      "        ...,\n",
      "        [-2.7065e-01, -4.4440e+00, -6.5611e-01,  ..., -8.3789e-05,\n",
      "          2.7957e+00,  1.5435e+00],\n",
      "        [-1.1525e+00, -3.3558e+00,  4.6301e-01,  ..., -8.3405e-02,\n",
      "          1.8668e+00,  1.0542e+00],\n",
      "        [-6.1211e-01, -2.8148e+00,  1.5674e+00,  ...,  9.6324e-02,\n",
      "          2.4449e+00,  2.0147e+00]], grad_fn=<SliceBackward0>)\n",
      "Decoder output\n",
      "tensor([[-1.1219, -2.9811, -1.4193,  ...,  0.0547, -0.0901,  0.3620],\n",
      "        [-1.0734, -2.7755, -0.3352,  ...,  0.0584,  0.2962,  0.9954],\n",
      "        [-0.9564, -2.6937, -0.6563,  ..., -0.2260, -0.3883,  0.4944],\n",
      "        ...,\n",
      "        [-0.4547, -3.2757, -0.7153,  ..., -0.2718,  1.6180,  0.7716],\n",
      "        [-1.0526, -2.5535,  0.0479,  ..., -0.3243,  1.0041,  0.4506],\n",
      "        [-0.7173, -2.2591,  0.8083,  ..., -0.2214,  1.4225,  1.1214]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Output\n",
      "tensor([[ 0.0310,  0.4832, -0.2951,  ...,  1.2988, -0.2955, -0.4847],\n",
      "        [ 0.2085,  0.5930, -0.3348,  ...,  0.7263, -0.0880, -0.3932],\n",
      "        [ 0.1833,  0.3796, -0.3958,  ...,  0.9673, -0.2616, -0.2159],\n",
      "        ...,\n",
      "        [ 0.1569,  0.3412,  0.1895,  ...,  1.0137, -0.1192, -0.2396],\n",
      "        [-0.0970,  0.6146,  0.1632,  ...,  0.7509, -0.0865, -0.1076],\n",
      "        [ 0.0771,  0.6744, -0.0240,  ...,  0.6644, -0.0337, -0.1764]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/dctrl/ma618/torch/lib64/python3.9/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "predictions = model(batch['ehr'],  batch['prev_cxr'], None, batch['attention_mask'], batch['attention_mask'], debug = True, batch_idx_debug = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7cf4cfe9-c9b1-4131-b29c-e75221dd2fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0310,  0.4832, -0.2951,  ...,  1.2988, -0.2955, -0.4847],\n",
       "        [ 0.2085,  0.5930, -0.3348,  ...,  0.7263, -0.0880, -0.3932],\n",
       "        [ 0.1833,  0.3796, -0.3958,  ...,  0.9673, -0.2616, -0.2159],\n",
       "        ...,\n",
       "        [ 0.1569,  0.3412,  0.1895,  ...,  1.0137, -0.1192, -0.2396],\n",
       "        [-0.0970,  0.6146,  0.1632,  ...,  0.7509, -0.0865, -0.1076],\n",
       "        [ 0.0771,  0.6744, -0.0240,  ...,  0.6644, -0.0337, -0.1764]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[4,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47ec1217-8f8d-44f2-96f9-ebe3a031657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = TrainerConfig(\n",
    "        max_epochs=configs['num_epochs'],\n",
    "        batch_size=configs['batch_size'],\n",
    "        learning_rate=configs['lr'],\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler=\"cosine\",\n",
    "        warmup_ratio=0.1,\n",
    "        checkpoint_dir= './',\n",
    "        save_every=5,\n",
    "        patience=15,\n",
    "        grad_norm_clip=1.0,\n",
    "        log_every=10,\n",
    "        eval_every=1,\n",
    "        mixed_precision=True,\n",
    "        teacher_forcing_ratio=0.5,\n",
    "        teacher_forcing_decay=0.98,  # Gradually reduce teacher forcing\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f0ce884-af85-4184-8326-0545f97a033a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3200694/2604809632.py:49: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=trainer_config.mixed_precision)\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "        \n",
    "# Create checkpoint directory\n",
    "\n",
    "# Initialize teacher forcing ratio\n",
    "teacher_forcing_ratio = trainer_config.teacher_forcing_ratio\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=trainer_config.learning_rate,\n",
    "    weight_decay=trainer_config.weight_decay,\n",
    ")\n",
    "\n",
    "# Setup learning rate scheduler\n",
    "total_steps = len(dataloader) * trainer_config.max_epochs\n",
    "warmup_steps = int(total_steps * trainer_config.warmup_ratio)\n",
    "\n",
    "if trainer_config.lr_scheduler == \"cosine\":\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=total_steps - warmup_steps\n",
    "    )\n",
    "elif trainer_config.lr_scheduler == \"linear\":\n",
    "    scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer, \n",
    "        start_factor=1.0,\n",
    "        end_factor=0.1,\n",
    "        total_iters=total_steps - warmup_steps\n",
    "    )\n",
    "elif trainer_config.lr_scheduler == \"constant\":\n",
    "    scheduler = torch.optim.lr_scheduler.ConstantLR(\n",
    "        optimizer, factor=1.0, total_iters=total_steps\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"Unknown scheduler: {trainer_config.lr_scheduler}\")\n",
    "\n",
    "# Warmup scheduler\n",
    "warmup_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "    optimizer,\n",
    "    start_factor=0.1,\n",
    "    end_factor=1.0,\n",
    "    total_iters=warmup_steps\n",
    ")\n",
    "\n",
    "# Setup loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Setup mixed precision training\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=trainer_config.mixed_precision)\n",
    "\n",
    "# Setup model\n",
    "model.to(device)\n",
    "\n",
    "# Tracking variables\n",
    "global_step = 0\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36aa3881-cbd4-4cba-9cd1-415264f8bba5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/2212 [00:00<?, ?batch/s]/tmp/ipykernel_3200694/2749420202.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=trainer_config.mixed_precision):\n",
      "Epoch 1:   0%|          | 1/2212 [00:04<2:30:02,  4.07s/batch, loss=5.97, lr=1.11e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 5.967195510864258, 'train/learning_rate': 1.1106690777576851e-05, 'train/global_step': 136}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 11/2212 [00:09<15:26,  2.38batch/s, loss=5.89, lr=1.12e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 5.893756866455078, 'train/learning_rate': 1.1188065099457504e-05, 'train/global_step': 146}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|          | 21/2212 [00:12<13:50,  2.64batch/s, loss=6.52, lr=1.13e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 6.5240983963012695, 'train/learning_rate': 1.1269439421338157e-05, 'train/global_step': 156}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|▏         | 31/2212 [00:16<11:05,  3.28batch/s, loss=5.68, lr=1.14e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 5.675389289855957, 'train/learning_rate': 1.1350813743218806e-05, 'train/global_step': 166}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   2%|▏         | 41/2212 [00:19<10:17,  3.51batch/s, loss=5.71, lr=1.14e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 5.712188720703125, 'train/learning_rate': 1.1432188065099458e-05, 'train/global_step': 176}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   2%|▏         | 51/2212 [00:22<12:50,  2.80batch/s, loss=5.2, lr=1.15e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 5.204791069030762, 'train/learning_rate': 1.1513562386980113e-05, 'train/global_step': 186}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   3%|▎         | 61/2212 [00:26<10:58,  3.27batch/s, loss=4.4, lr=1.16e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 4.395272731781006, 'train/learning_rate': 1.1594936708860759e-05, 'train/global_step': 196}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   3%|▎         | 71/2212 [00:29<10:32,  3.38batch/s, loss=4.82, lr=1.17e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 4.817283630371094, 'train/learning_rate': 1.167631103074141e-05, 'train/global_step': 206}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   4%|▎         | 81/2212 [00:32<10:12,  3.48batch/s, loss=6.53, lr=1.18e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 6.530384063720703, 'train/learning_rate': 1.1757685352622057e-05, 'train/global_step': 216}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   4%|▍         | 91/2212 [00:35<09:23,  3.76batch/s, loss=6.03, lr=1.18e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 6.025198936462402, 'train/learning_rate': 1.1839059674502706e-05, 'train/global_step': 226}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   5%|▍         | 101/2212 [00:38<10:05,  3.48batch/s, loss=5.49, lr=1.19e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 5.487472057342529, 'train/learning_rate': 1.1920433996383356e-05, 'train/global_step': 236}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   5%|▌         | 111/2212 [00:42<11:46,  2.97batch/s, loss=5.67, lr=1.2e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 5.666959285736084, 'train/learning_rate': 1.2001808318264005e-05, 'train/global_step': 246}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   5%|▌         | 121/2212 [00:44<09:40,  3.60batch/s, loss=5.35, lr=1.21e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 5.347719669342041, 'train/learning_rate': 1.2083182640144654e-05, 'train/global_step': 256}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   6%|▌         | 131/2212 [00:48<10:05,  3.44batch/s, loss=5.65, lr=1.22e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 5.652981758117676, 'train/learning_rate': 1.2164556962025307e-05, 'train/global_step': 266}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   6%|▋         | 141/2212 [00:51<10:23,  3.32batch/s, loss=5.18, lr=1.22e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 5.1848955154418945, 'train/learning_rate': 1.2245931283905956e-05, 'train/global_step': 276}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   7%|▋         | 151/2212 [00:54<11:27,  3.00batch/s, loss=4.76, lr=1.23e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 4.7589111328125, 'train/learning_rate': 1.2327305605786602e-05, 'train/global_step': 286}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   7%|▋         | 161/2212 [00:57<10:59,  3.11batch/s, loss=4.41, lr=1.24e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 4.4064507484436035, 'train/learning_rate': 1.2408679927667254e-05, 'train/global_step': 296}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   8%|▊         | 171/2212 [01:00<10:40,  3.18batch/s, loss=3.34, lr=1.25e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 3.342633008956909, 'train/learning_rate': 1.2490054249547902e-05, 'train/global_step': 306}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   8%|▊         | 181/2212 [01:04<11:31,  2.94batch/s, loss=3.25, lr=1.26e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 3.246659278869629, 'train/learning_rate': 1.2571428571428551e-05, 'train/global_step': 316}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   9%|▊         | 191/2212 [01:07<09:23,  3.59batch/s, loss=5.16, lr=1.27e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 5.155475616455078, 'train/learning_rate': 1.2652802893309199e-05, 'train/global_step': 326}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   9%|▉         | 201/2212 [01:09<09:11,  3.65batch/s, loss=4.2, lr=1.27e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 4.199784755706787, 'train/learning_rate': 1.2734177215189846e-05, 'train/global_step': 336}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  10%|▉         | 211/2212 [01:12<09:07,  3.65batch/s, loss=4.39, lr=1.28e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 4.388845443725586, 'train/learning_rate': 1.2815551537070499e-05, 'train/global_step': 346}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  10%|▉         | 221/2212 [01:15<09:05,  3.65batch/s, loss=4.24, lr=1.29e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 4.242391586303711, 'train/learning_rate': 1.2896925858951147e-05, 'train/global_step': 356}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  10%|█         | 231/2212 [01:18<09:04,  3.64batch/s, loss=3.57, lr=1.3e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 3.57267427444458, 'train/learning_rate': 1.2978300180831804e-05, 'train/global_step': 366}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  11%|█         | 241/2212 [01:21<11:30,  2.86batch/s, loss=3.18, lr=1.31e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 3.177051067352295, 'train/learning_rate': 1.3059674502712459e-05, 'train/global_step': 376}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  11%|█▏        | 251/2212 [01:24<10:19,  3.16batch/s, loss=4.46, lr=1.31e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 4.460968017578125, 'train/learning_rate': 1.3141048824593106e-05, 'train/global_step': 386}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  12%|█▏        | 261/2212 [01:27<09:31,  3.42batch/s, loss=4.7, lr=1.32e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 4.7039947509765625, 'train/learning_rate': 1.3222423146473755e-05, 'train/global_step': 396}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  12%|█▏        | 271/2212 [01:30<08:59,  3.60batch/s, loss=3.39, lr=1.33e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 3.3918535709381104, 'train/learning_rate': 1.3303797468354401e-05, 'train/global_step': 406}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  13%|█▎        | 281/2212 [01:33<10:09,  3.17batch/s, loss=3.88, lr=1.34e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 3.876826286315918, 'train/learning_rate': 1.3385171790235052e-05, 'train/global_step': 416}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  13%|█▎        | 291/2212 [01:36<09:50,  3.25batch/s, loss=3.19, lr=1.35e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 3.187915563583374, 'train/learning_rate': 1.3466546112115705e-05, 'train/global_step': 426}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  14%|█▎        | 301/2212 [01:39<12:09,  2.62batch/s, loss=3.19, lr=1.35e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 3.1895246505737305, 'train/learning_rate': 1.3547920433996354e-05, 'train/global_step': 436}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  14%|█▍        | 311/2212 [01:42<09:29,  3.34batch/s, loss=3.99, lr=1.36e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 3.990095853805542, 'train/learning_rate': 1.3629294755877003e-05, 'train/global_step': 446}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  15%|█▍        | 321/2212 [01:45<09:50,  3.20batch/s, loss=3.89, lr=1.37e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 3.891892433166504, 'train/learning_rate': 1.3710669077757647e-05, 'train/global_step': 456}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  15%|█▍        | 331/2212 [01:48<08:51,  3.54batch/s, loss=3.94, lr=1.38e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 3.9417285919189453, 'train/learning_rate': 1.3792043399638298e-05, 'train/global_step': 466}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  15%|█▌        | 341/2212 [01:52<08:56,  3.48batch/s, loss=3.85, lr=1.39e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 3.846552848815918, 'train/learning_rate': 1.3873417721518946e-05, 'train/global_step': 476}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  16%|█▌        | 351/2212 [01:55<08:48,  3.52batch/s, loss=3.89, lr=1.4e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 3.8945860862731934, 'train/learning_rate': 1.3954792043399597e-05, 'train/global_step': 486}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  16%|█▋        | 361/2212 [01:57<08:19,  3.71batch/s, loss=3.76, lr=1.4e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 3.760141611099243, 'train/learning_rate': 1.403616636528025e-05, 'train/global_step': 496}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  17%|█▋        | 371/2212 [02:01<10:39,  2.88batch/s, loss=3.15, lr=1.41e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 3.151137590408325, 'train/learning_rate': 1.41175406871609e-05, 'train/global_step': 506}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  17%|█▋        | 381/2212 [02:04<09:15,  3.29batch/s, loss=2.92, lr=1.42e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 2.9232776165008545, 'train/learning_rate': 1.4198915009041551e-05, 'train/global_step': 516}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  18%|█▊        | 391/2212 [02:07<10:06,  3.00batch/s, loss=3.91, lr=1.43e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 3.913337469100952, 'train/learning_rate': 1.4280289330922206e-05, 'train/global_step': 526}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  18%|█▊        | 401/2212 [02:10<08:33,  3.53batch/s, loss=3.76, lr=1.44e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 3.7635393142700195, 'train/learning_rate': 1.4361663652802862e-05, 'train/global_step': 536}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  19%|█▊        | 411/2212 [02:13<08:45,  3.43batch/s, loss=3.64, lr=1.44e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 3.6407220363616943, 'train/learning_rate': 1.4443037974683511e-05, 'train/global_step': 546}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  19%|█▉        | 421/2212 [02:16<08:32,  3.49batch/s, loss=2.9, lr=1.45e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 2.8981869220733643, 'train/learning_rate': 1.4524412296564164e-05, 'train/global_step': 556}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  19%|█▉        | 431/2212 [02:19<09:04,  3.27batch/s, loss=2.69, lr=1.46e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 2.689687490463257, 'train/learning_rate': 1.4605786618444811e-05, 'train/global_step': 566}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  20%|█▉        | 441/2212 [02:22<09:16,  3.18batch/s, loss=3.2, lr=1.47e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 3.2041375637054443, 'train/learning_rate': 1.4687160940325457e-05, 'train/global_step': 576}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  20%|██        | 451/2212 [02:25<07:58,  3.68batch/s, loss=2.8, lr=1.48e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 2.8044557571411133, 'train/learning_rate': 1.4768535262206103e-05, 'train/global_step': 586}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  21%|██        | 461/2212 [02:28<09:22,  3.11batch/s, loss=3.05, lr=1.48e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 3.0477161407470703, 'train/learning_rate': 1.4849909584086754e-05, 'train/global_step': 596}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  21%|██▏       | 471/2212 [02:31<10:21,  2.80batch/s, loss=3.17, lr=1.49e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 3.1690409183502197, 'train/learning_rate': 1.4931283905967412e-05, 'train/global_step': 606}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  22%|██▏       | 481/2212 [02:35<12:04,  2.39batch/s, loss=2.93, lr=1.5e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 2.9284048080444336, 'train/learning_rate': 1.5012658227848066e-05, 'train/global_step': 616}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  22%|██▏       | 491/2212 [02:38<08:40,  3.31batch/s, loss=2.66, lr=1.51e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 2.663022518157959, 'train/learning_rate': 1.5094032549728712e-05, 'train/global_step': 626}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  23%|██▎       | 501/2212 [02:41<08:11,  3.48batch/s, loss=2.91, lr=1.52e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 2.912884473800659, 'train/learning_rate': 1.517540687160937e-05, 'train/global_step': 636}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  23%|██▎       | 511/2212 [02:44<08:47,  3.23batch/s, loss=1.89, lr=1.53e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.894087553024292, 'train/learning_rate': 1.5256781193490024e-05, 'train/global_step': 646}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  24%|██▎       | 521/2212 [02:47<07:59,  3.52batch/s, loss=2.88, lr=1.53e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 2.880065679550171, 'train/learning_rate': 1.5338155515370676e-05, 'train/global_step': 656}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  24%|██▍       | 531/2212 [02:50<09:59,  2.80batch/s, loss=2.18, lr=1.54e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 2.177847146987915, 'train/learning_rate': 1.5419529837251322e-05, 'train/global_step': 666}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  24%|██▍       | 541/2212 [02:53<07:59,  3.48batch/s, loss=2.55, lr=1.55e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 2.549988031387329, 'train/learning_rate': 1.5500904159131968e-05, 'train/global_step': 676}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  25%|██▍       | 551/2212 [02:57<12:00,  2.31batch/s, loss=2.8, lr=1.56e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 2.797680139541626, 'train/learning_rate': 1.5582278481012614e-05, 'train/global_step': 686}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  25%|██▌       | 561/2212 [03:00<09:08,  3.01batch/s, loss=2.24, lr=1.57e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 2.242486000061035, 'train/learning_rate': 1.566365280289326e-05, 'train/global_step': 696}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  26%|██▌       | 571/2212 [03:04<08:17,  3.30batch/s, loss=2.3, lr=1.57e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 2.3042643070220947, 'train/learning_rate': 1.574502712477391e-05, 'train/global_step': 706}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  26%|██▋       | 581/2212 [03:07<07:49,  3.47batch/s, loss=2.87, lr=1.58e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 2.8668417930603027, 'train/learning_rate': 1.582640144665457e-05, 'train/global_step': 716}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  27%|██▋       | 591/2212 [03:10<08:41,  3.11batch/s, loss=3.12, lr=1.59e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 3.123838186264038, 'train/learning_rate': 1.590777576853522e-05, 'train/global_step': 726}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  27%|██▋       | 601/2212 [03:13<07:49,  3.43batch/s, loss=2.28, lr=1.6e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 2.2801268100738525, 'train/learning_rate': 1.598915009041587e-05, 'train/global_step': 736}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  28%|██▊       | 611/2212 [03:17<09:38,  2.77batch/s, loss=2.1, lr=1.61e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 2.0968313217163086, 'train/learning_rate': 1.6070524412296523e-05, 'train/global_step': 746}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  28%|██▊       | 621/2212 [03:20<10:00,  2.65batch/s, loss=1.64, lr=1.62e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.635006308555603, 'train/learning_rate': 1.615189873417717e-05, 'train/global_step': 756}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  29%|██▊       | 631/2212 [03:23<08:51,  2.97batch/s, loss=2.95, lr=1.62e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 2.9500157833099365, 'train/learning_rate': 1.6233273056057818e-05, 'train/global_step': 766}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  29%|██▉       | 641/2212 [03:27<07:53,  3.32batch/s, loss=2.57, lr=1.63e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 2.5667288303375244, 'train/learning_rate': 1.6314647377938464e-05, 'train/global_step': 776}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  29%|██▉       | 651/2212 [03:30<07:20,  3.54batch/s, loss=2.02, lr=1.64e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 2.016770601272583, 'train/learning_rate': 1.639602169981912e-05, 'train/global_step': 786}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  30%|██▉       | 661/2212 [03:32<07:19,  3.53batch/s, loss=2.2, lr=1.65e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 2.2004778385162354, 'train/learning_rate': 1.6477396021699776e-05, 'train/global_step': 796}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  30%|███       | 671/2212 [03:36<09:25,  2.73batch/s, loss=2.19, lr=1.66e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 2.1855506896972656, 'train/learning_rate': 1.6558770343580425e-05, 'train/global_step': 806}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  31%|███       | 681/2212 [03:39<07:25,  3.43batch/s, loss=1.96, lr=1.66e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.9568480253219604, 'train/learning_rate': 1.664014466546108e-05, 'train/global_step': 816}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  31%|███       | 691/2212 [03:43<09:56,  2.55batch/s, loss=1.89, lr=1.67e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.8905129432678223, 'train/learning_rate': 1.672151898734173e-05, 'train/global_step': 826}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  32%|███▏      | 701/2212 [03:46<08:45,  2.88batch/s, loss=2.11, lr=1.68e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 2.1069343090057373, 'train/learning_rate': 1.6802893309222387e-05, 'train/global_step': 836}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  32%|███▏      | 711/2212 [03:50<09:29,  2.63batch/s, loss=2, lr=1.69e-5]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 2.003554344177246, 'train/learning_rate': 1.6884267631103036e-05, 'train/global_step': 846}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  33%|███▎      | 721/2212 [03:53<07:45,  3.20batch/s, loss=2.61, lr=1.7e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 2.6100096702575684, 'train/learning_rate': 1.696564195298369e-05, 'train/global_step': 856}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  33%|███▎      | 731/2212 [03:56<07:33,  3.27batch/s, loss=2.75, lr=1.7e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 2.7528066635131836, 'train/learning_rate': 1.7047016274864338e-05, 'train/global_step': 866}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  33%|███▎      | 741/2212 [03:59<08:09,  3.01batch/s, loss=1.95, lr=1.71e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.948580026626587, 'train/learning_rate': 1.712839059674498e-05, 'train/global_step': 876}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  34%|███▍      | 751/2212 [04:03<07:52,  3.09batch/s, loss=1.62, lr=1.72e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.6207503080368042, 'train/learning_rate': 1.7209764918625636e-05, 'train/global_step': 886}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  34%|███▍      | 761/2212 [04:06<07:08,  3.39batch/s, loss=2.41, lr=1.73e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 2.4105887413024902, 'train/learning_rate': 1.7291139240506296e-05, 'train/global_step': 896}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  35%|███▍      | 771/2212 [04:09<08:51,  2.71batch/s, loss=1.94, lr=1.74e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.9409290552139282, 'train/learning_rate': 1.7372513562386945e-05, 'train/global_step': 906}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  35%|███▌      | 781/2212 [04:12<07:01,  3.40batch/s, loss=2.04, lr=1.75e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 2.0360231399536133, 'train/learning_rate': 1.7453887884267598e-05, 'train/global_step': 916}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  36%|███▌      | 791/2212 [04:15<06:35,  3.59batch/s, loss=1.86, lr=1.75e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.8569406270980835, 'train/learning_rate': 1.753526220614825e-05, 'train/global_step': 926}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  36%|███▌      | 801/2212 [04:18<06:56,  3.38batch/s, loss=1.85, lr=1.76e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.849409580230713, 'train/learning_rate': 1.7616636528028893e-05, 'train/global_step': 936}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  37%|███▋      | 811/2212 [04:21<07:24,  3.15batch/s, loss=2.43, lr=1.77e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 2.426612377166748, 'train/learning_rate': 1.769801084990955e-05, 'train/global_step': 946}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  37%|███▋      | 821/2212 [04:24<08:42,  2.66batch/s, loss=1.11, lr=1.78e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.1074830293655396, 'train/learning_rate': 1.77793851717902e-05, 'train/global_step': 956}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  38%|███▊      | 831/2212 [04:28<06:47,  3.39batch/s, loss=1.65, lr=1.79e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.645728588104248, 'train/learning_rate': 1.786075949367084e-05, 'train/global_step': 966}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  38%|███▊      | 841/2212 [04:31<08:59,  2.54batch/s, loss=1.96, lr=1.79e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.9578452110290527, 'train/learning_rate': 1.794213381555149e-05, 'train/global_step': 976}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  38%|███▊      | 851/2212 [04:34<07:58,  2.84batch/s, loss=2.21, lr=1.8e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 2.2148468494415283, 'train/learning_rate': 1.802350813743214e-05, 'train/global_step': 986}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  39%|███▉      | 861/2212 [04:38<06:44,  3.34batch/s, loss=1.39, lr=1.81e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.3931869268417358, 'train/learning_rate': 1.8104882459312788e-05, 'train/global_step': 996}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  39%|███▉      | 871/2212 [04:41<07:45,  2.88batch/s, loss=2.64, lr=1.82e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 2.641312837600708, 'train/learning_rate': 1.8186256781193437e-05, 'train/global_step': 1006}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  40%|███▉      | 881/2212 [04:44<08:23,  2.64batch/s, loss=1.68, lr=1.83e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.678617238998413, 'train/learning_rate': 1.8267631103074083e-05, 'train/global_step': 1016}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  40%|████      | 891/2212 [04:48<07:52,  2.79batch/s, loss=2.16, lr=1.83e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 2.1560802459716797, 'train/learning_rate': 1.8349005424954743e-05, 'train/global_step': 1026}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  41%|████      | 901/2212 [04:51<07:46,  2.81batch/s, loss=1.66, lr=1.84e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.656196117401123, 'train/learning_rate': 1.843037974683539e-05, 'train/global_step': 1036}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  41%|████      | 911/2212 [04:55<07:07,  3.04batch/s, loss=1.46, lr=1.85e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.4575883150100708, 'train/learning_rate': 1.8511754068716045e-05, 'train/global_step': 1046}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  42%|████▏     | 921/2212 [04:58<06:47,  3.17batch/s, loss=2.15, lr=1.86e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 2.151710271835327, 'train/learning_rate': 1.859312839059669e-05, 'train/global_step': 1056}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  42%|████▏     | 931/2212 [05:01<07:07,  3.00batch/s, loss=1.31, lr=1.87e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.3117010593414307, 'train/learning_rate': 1.8674502712477343e-05, 'train/global_step': 1066}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  43%|████▎     | 941/2212 [05:05<07:22,  2.87batch/s, loss=1.52, lr=1.88e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.520783543586731, 'train/learning_rate': 1.8755877034358e-05, 'train/global_step': 1076}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  43%|████▎     | 951/2212 [05:08<06:50,  3.07batch/s, loss=1.51, lr=1.88e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.509740948677063, 'train/learning_rate': 1.883725135623865e-05, 'train/global_step': 1086}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  43%|████▎     | 961/2212 [05:11<05:54,  3.53batch/s, loss=1.7, lr=1.89e-5]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.700475811958313, 'train/learning_rate': 1.8918625678119304e-05, 'train/global_step': 1096}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  44%|████▍     | 971/2212 [05:14<06:49,  3.03batch/s, loss=1.41, lr=1.9e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.4062010049819946, 'train/learning_rate': 1.899999999999995e-05, 'train/global_step': 1106}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  44%|████▍     | 981/2212 [05:18<09:36,  2.14batch/s, loss=1.4, lr=1.91e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.4043090343475342, 'train/learning_rate': 1.9081374321880593e-05, 'train/global_step': 1116}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  45%|████▍     | 991/2212 [05:21<06:03,  3.36batch/s, loss=1.42, lr=1.92e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.4174810647964478, 'train/learning_rate': 1.916274864376124e-05, 'train/global_step': 1126}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  45%|████▌     | 1001/2212 [05:25<07:07,  2.84batch/s, loss=1.39, lr=1.92e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.3876947164535522, 'train/learning_rate': 1.924412296564189e-05, 'train/global_step': 1136}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  46%|████▌     | 1011/2212 [05:28<06:29,  3.09batch/s, loss=1.97, lr=1.93e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.965153455734253, 'train/learning_rate': 1.9325497287522544e-05, 'train/global_step': 1146}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  46%|████▌     | 1021/2212 [05:31<05:40,  3.50batch/s, loss=1.36, lr=1.94e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.3603887557983398, 'train/learning_rate': 1.9406871609403193e-05, 'train/global_step': 1156}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  47%|████▋     | 1031/2212 [05:34<06:06,  3.23batch/s, loss=1.48, lr=1.95e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.4753010272979736, 'train/learning_rate': 1.948824593128385e-05, 'train/global_step': 1166}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  47%|████▋     | 1041/2212 [05:37<05:32,  3.52batch/s, loss=1.67, lr=1.96e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.6733242273330688, 'train/learning_rate': 1.95696202531645e-05, 'train/global_step': 1176}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  48%|████▊     | 1051/2212 [05:40<06:18,  3.07batch/s, loss=1.67, lr=1.97e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.6650854349136353, 'train/learning_rate': 1.9650994575045148e-05, 'train/global_step': 1186}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  48%|████▊     | 1061/2212 [05:43<05:44,  3.34batch/s, loss=2.04, lr=1.97e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 2.0379467010498047, 'train/learning_rate': 1.9732368896925797e-05, 'train/global_step': 1196}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  48%|████▊     | 1071/2212 [05:47<05:48,  3.28batch/s, loss=1.68, lr=1.98e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.6787199974060059, 'train/learning_rate': 1.9813743218806453e-05, 'train/global_step': 1206}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  49%|████▉     | 1081/2212 [05:50<06:18,  2.98batch/s, loss=1.87, lr=1.99e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.8678960800170898, 'train/learning_rate': 1.9895117540687095e-05, 'train/global_step': 1216}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  49%|████▉     | 1091/2212 [05:53<06:29,  2.88batch/s, loss=1.86, lr=2e-5]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.8628824949264526, 'train/learning_rate': 1.9976491862567745e-05, 'train/global_step': 1226}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  50%|████▉     | 1101/2212 [05:57<07:24,  2.50batch/s, loss=1.87, lr=2.01e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.8710132837295532, 'train/learning_rate': 2.0057866184448394e-05, 'train/global_step': 1236}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  50%|█████     | 1111/2212 [06:00<05:05,  3.61batch/s, loss=1.89, lr=2.01e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.8886616230010986, 'train/learning_rate': 2.013924050632904e-05, 'train/global_step': 1246}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  51%|█████     | 1121/2212 [06:03<05:54,  3.08batch/s, loss=1.95, lr=2.02e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.9458777904510498, 'train/learning_rate': 2.0220614828209686e-05, 'train/global_step': 1256}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  51%|█████     | 1131/2212 [06:06<05:54,  3.05batch/s, loss=1.18, lr=2.03e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.175872564315796, 'train/learning_rate': 2.0301989150090335e-05, 'train/global_step': 1266}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  52%|█████▏    | 1141/2212 [06:10<05:29,  3.25batch/s, loss=1.47, lr=2.04e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.469742774963379, 'train/learning_rate': 2.038336347197098e-05, 'train/global_step': 1276}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  52%|█████▏    | 1151/2212 [06:13<05:55,  2.99batch/s, loss=2.03, lr=2.05e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 2.0340871810913086, 'train/learning_rate': 2.046473779385163e-05, 'train/global_step': 1286}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  52%|█████▏    | 1161/2212 [06:16<06:04,  2.88batch/s, loss=1.06, lr=2.05e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.0551247596740723, 'train/learning_rate': 2.0546112115732286e-05, 'train/global_step': 1296}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  53%|█████▎    | 1171/2212 [06:20<05:40,  3.05batch/s, loss=1.29, lr=2.06e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.2911087274551392, 'train/learning_rate': 2.0627486437612932e-05, 'train/global_step': 1306}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  53%|█████▎    | 1181/2212 [06:23<05:08,  3.34batch/s, loss=1.73, lr=2.07e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.7304563522338867, 'train/learning_rate': 2.0708860759493578e-05, 'train/global_step': 1316}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  54%|█████▍    | 1191/2212 [06:26<04:42,  3.61batch/s, loss=1.44, lr=2.08e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.4392448663711548, 'train/learning_rate': 2.079023508137423e-05, 'train/global_step': 1326}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  54%|█████▍    | 1201/2212 [06:29<05:00,  3.36batch/s, loss=1.59, lr=2.09e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.5934921503067017, 'train/learning_rate': 2.0871609403254883e-05, 'train/global_step': 1336}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  55%|█████▍    | 1211/2212 [06:32<05:25,  3.08batch/s, loss=1.51, lr=2.1e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.506731390953064, 'train/learning_rate': 2.095298372513554e-05, 'train/global_step': 1346}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  55%|█████▌    | 1221/2212 [06:35<05:24,  3.05batch/s, loss=1.22, lr=2.1e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.2158914804458618, 'train/learning_rate': 2.103435804701618e-05, 'train/global_step': 1356}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  56%|█████▌    | 1231/2212 [06:39<07:01,  2.33batch/s, loss=1.5, lr=2.11e-5]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.5040562152862549, 'train/learning_rate': 2.1115732368896827e-05, 'train/global_step': 1366}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  56%|█████▌    | 1241/2212 [06:41<04:36,  3.51batch/s, loss=1.83, lr=2.12e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.8291205167770386, 'train/learning_rate': 2.119710669077748e-05, 'train/global_step': 1376}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  57%|█████▋    | 1251/2212 [06:45<04:43,  3.39batch/s, loss=1.55, lr=2.13e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.5490059852600098, 'train/learning_rate': 2.1278481012658136e-05, 'train/global_step': 1386}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  57%|█████▋    | 1261/2212 [06:48<04:46,  3.32batch/s, loss=1.48, lr=2.14e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.4773626327514648, 'train/learning_rate': 2.135985533453879e-05, 'train/global_step': 1396}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  57%|█████▋    | 1271/2212 [06:51<05:47,  2.71batch/s, loss=1.38, lr=2.14e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.3817486763000488, 'train/learning_rate': 2.1441229656419434e-05, 'train/global_step': 1406}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  58%|█████▊    | 1281/2212 [06:54<04:49,  3.21batch/s, loss=1.25, lr=2.15e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.253340721130371, 'train/learning_rate': 2.152260397830008e-05, 'train/global_step': 1416}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  58%|█████▊    | 1291/2212 [06:57<04:25,  3.46batch/s, loss=1.19, lr=2.16e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.192317008972168, 'train/learning_rate': 2.1603978300180726e-05, 'train/global_step': 1426}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  59%|█████▉    | 1301/2212 [07:00<04:33,  3.33batch/s, loss=1.4, lr=2.17e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.3968641757965088, 'train/learning_rate': 2.168535262206138e-05, 'train/global_step': 1436}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  59%|█████▉    | 1311/2212 [07:03<04:23,  3.41batch/s, loss=0.995, lr=2.18e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.9950806498527527, 'train/learning_rate': 2.1766726943942035e-05, 'train/global_step': 1446}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  60%|█████▉    | 1321/2212 [07:07<06:52,  2.16batch/s, loss=1.44, lr=2.18e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.440327763557434, 'train/learning_rate': 2.1848101265822687e-05, 'train/global_step': 1456}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  60%|██████    | 1331/2212 [07:10<04:39,  3.15batch/s, loss=1.44, lr=2.19e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.4405014514923096, 'train/learning_rate': 2.1929475587703337e-05, 'train/global_step': 1466}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  61%|██████    | 1341/2212 [07:13<05:15,  2.76batch/s, loss=1.06, lr=2.2e-5]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.0604562759399414, 'train/learning_rate': 2.2010849909583993e-05, 'train/global_step': 1476}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  61%|██████    | 1351/2212 [07:17<04:12,  3.40batch/s, loss=1.44, lr=2.21e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.4389294385910034, 'train/learning_rate': 2.209222423146465e-05, 'train/global_step': 1486}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  62%|██████▏   | 1361/2212 [07:20<04:37,  3.06batch/s, loss=0.764, lr=2.22e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.7644575238227844, 'train/learning_rate': 2.21735985533453e-05, 'train/global_step': 1496}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  62%|██████▏   | 1371/2212 [07:23<04:02,  3.46batch/s, loss=1.55, lr=2.23e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.5533204078674316, 'train/learning_rate': 2.225497287522596e-05, 'train/global_step': 1506}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  62%|██████▏   | 1381/2212 [07:26<03:55,  3.53batch/s, loss=1.43, lr=2.23e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.4299813508987427, 'train/learning_rate': 2.233634719710661e-05, 'train/global_step': 1516}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  63%|██████▎   | 1391/2212 [07:29<04:10,  3.27batch/s, loss=1.11, lr=2.24e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.1070713996887207, 'train/learning_rate': 2.241772151898726e-05, 'train/global_step': 1526}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  63%|██████▎   | 1401/2212 [07:32<03:49,  3.54batch/s, loss=0.918, lr=2.25e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.9175230264663696, 'train/learning_rate': 2.249909584086792e-05, 'train/global_step': 1536}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  64%|██████▍   | 1411/2212 [07:36<04:24,  3.03batch/s, loss=1.46, lr=2.26e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.4634599685668945, 'train/learning_rate': 2.2580470162748578e-05, 'train/global_step': 1546}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  64%|██████▍   | 1421/2212 [07:39<03:51,  3.42batch/s, loss=1.18, lr=2.27e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.176597237586975, 'train/learning_rate': 2.2661844484629234e-05, 'train/global_step': 1556}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  65%|██████▍   | 1431/2212 [07:42<04:15,  3.06batch/s, loss=1.02, lr=2.27e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.0195099115371704, 'train/learning_rate': 2.2743218806509877e-05, 'train/global_step': 1566}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  65%|██████▌   | 1441/2212 [07:45<03:35,  3.57batch/s, loss=1.42, lr=2.28e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.424550175666809, 'train/learning_rate': 2.282459312839053e-05, 'train/global_step': 1576}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  66%|██████▌   | 1451/2212 [07:48<04:04,  3.11batch/s, loss=1.04, lr=2.29e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.0444648265838623, 'train/learning_rate': 2.2905967450271172e-05, 'train/global_step': 1586}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  66%|██████▌   | 1461/2212 [07:52<05:05,  2.46batch/s, loss=1.27, lr=2.3e-5]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.2672539949417114, 'train/learning_rate': 2.2987341772151824e-05, 'train/global_step': 1596}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  67%|██████▋   | 1471/2212 [07:55<04:03,  3.04batch/s, loss=1.2, lr=2.31e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.1976675987243652, 'train/learning_rate': 2.306871609403247e-05, 'train/global_step': 1606}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  67%|██████▋   | 1481/2212 [07:59<04:11,  2.91batch/s, loss=1.18, lr=2.32e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.175409197807312, 'train/learning_rate': 2.3150090415913126e-05, 'train/global_step': 1616}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  67%|██████▋   | 1491/2212 [08:02<03:49,  3.14batch/s, loss=1.07, lr=2.32e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.071778416633606, 'train/learning_rate': 2.3231464737793782e-05, 'train/global_step': 1626}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  68%|██████▊   | 1501/2212 [08:06<03:51,  3.07batch/s, loss=0.973, lr=2.33e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.9734837412834167, 'train/learning_rate': 2.331283905967444e-05, 'train/global_step': 1636}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  68%|██████▊   | 1511/2212 [08:09<04:25,  2.64batch/s, loss=1.24, lr=2.34e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.2428432703018188, 'train/learning_rate': 2.3394213381555088e-05, 'train/global_step': 1646}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  69%|██████▉   | 1521/2212 [08:12<04:07,  2.79batch/s, loss=0.987, lr=2.35e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.987263023853302, 'train/learning_rate': 2.3475587703435737e-05, 'train/global_step': 1656}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  69%|██████▉   | 1531/2212 [08:15<03:29,  3.25batch/s, loss=1.46, lr=2.36e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.4557534456253052, 'train/learning_rate': 2.3556962025316396e-05, 'train/global_step': 1666}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  70%|██████▉   | 1541/2212 [08:18<03:26,  3.26batch/s, loss=1.29, lr=2.36e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.2862331867218018, 'train/learning_rate': 2.3638336347197045e-05, 'train/global_step': 1676}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  70%|███████   | 1551/2212 [08:22<03:44,  2.95batch/s, loss=0.64, lr=2.37e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.6396257281303406, 'train/learning_rate': 2.3719710669077705e-05, 'train/global_step': 1686}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  71%|███████   | 1561/2212 [08:25<03:09,  3.43batch/s, loss=1.11, lr=2.38e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.1120820045471191, 'train/learning_rate': 2.3801084990958347e-05, 'train/global_step': 1696}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  71%|███████   | 1571/2212 [08:28<03:35,  2.98batch/s, loss=1.59, lr=2.39e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.5942089557647705, 'train/learning_rate': 2.3882459312838993e-05, 'train/global_step': 1706}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  71%|███████▏  | 1581/2212 [08:31<03:03,  3.43batch/s, loss=1.12, lr=2.4e-5]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.1209536790847778, 'train/learning_rate': 2.3963833634719636e-05, 'train/global_step': 1716}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  72%|███████▏  | 1591/2212 [08:34<03:05,  3.34batch/s, loss=0.976, lr=2.4e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.9760858416557312, 'train/learning_rate': 2.4045207956600288e-05, 'train/global_step': 1726}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  72%|███████▏  | 1601/2212 [08:37<03:13,  3.16batch/s, loss=1.19, lr=2.41e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.1866583824157715, 'train/learning_rate': 2.412658227848094e-05, 'train/global_step': 1736}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  73%|███████▎  | 1611/2212 [08:40<03:01,  3.31batch/s, loss=1.41, lr=2.42e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.4066531658172607, 'train/learning_rate': 2.4207956600361587e-05, 'train/global_step': 1746}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  73%|███████▎  | 1621/2212 [08:43<02:52,  3.43batch/s, loss=0.984, lr=2.43e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.9844028949737549, 'train/learning_rate': 2.4289330922242236e-05, 'train/global_step': 1756}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  74%|███████▎  | 1631/2212 [08:47<03:53,  2.48batch/s, loss=1.08, lr=2.44e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.0825088024139404, 'train/learning_rate': 2.4370705244122885e-05, 'train/global_step': 1766}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  74%|███████▍  | 1641/2212 [08:51<03:58,  2.40batch/s, loss=1.29, lr=2.45e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.2939893007278442, 'train/learning_rate': 2.445207956600354e-05, 'train/global_step': 1776}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  75%|███████▍  | 1651/2212 [08:54<03:22,  2.77batch/s, loss=1.02, lr=2.45e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.015323519706726, 'train/learning_rate': 2.4533453887884187e-05, 'train/global_step': 1786}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  75%|███████▌  | 1661/2212 [08:58<03:20,  2.75batch/s, loss=1.06, lr=2.46e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.0578553676605225, 'train/learning_rate': 2.461482820976483e-05, 'train/global_step': 1796}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  76%|███████▌  | 1671/2212 [09:01<02:39,  3.39batch/s, loss=1.15, lr=2.47e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.1470848321914673, 'train/learning_rate': 2.469620253164549e-05, 'train/global_step': 1806}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  76%|███████▌  | 1681/2212 [09:04<03:15,  2.71batch/s, loss=1.32, lr=2.48e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.319627046585083, 'train/learning_rate': 2.4777576853526138e-05, 'train/global_step': 1816}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  76%|███████▋  | 1691/2212 [09:07<02:35,  3.34batch/s, loss=0.842, lr=2.49e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.8421593308448792, 'train/learning_rate': 2.4858951175406784e-05, 'train/global_step': 1826}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  77%|███████▋  | 1701/2212 [09:11<02:25,  3.52batch/s, loss=0.944, lr=2.49e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.9439439177513123, 'train/learning_rate': 2.494032549728743e-05, 'train/global_step': 1836}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  77%|███████▋  | 1711/2212 [09:13<02:20,  3.56batch/s, loss=0.684, lr=2.5e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.6840324997901917, 'train/learning_rate': 2.502169981916808e-05, 'train/global_step': 1846}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  78%|███████▊  | 1721/2212 [09:16<02:13,  3.67batch/s, loss=0.711, lr=2.51e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.7111217379570007, 'train/learning_rate': 2.5103074141048735e-05, 'train/global_step': 1856}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  78%|███████▊  | 1731/2212 [09:20<02:42,  2.95batch/s, loss=0.898, lr=2.52e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.8984262347221375, 'train/learning_rate': 2.5184448462929388e-05, 'train/global_step': 1866}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  79%|███████▊  | 1741/2212 [09:24<02:56,  2.67batch/s, loss=0.95, lr=2.53e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.95046067237854, 'train/learning_rate': 2.5265822784810034e-05, 'train/global_step': 1876}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  79%|███████▉  | 1751/2212 [09:27<02:26,  3.15batch/s, loss=0.925, lr=2.53e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.9250343441963196, 'train/learning_rate': 2.5347197106690683e-05, 'train/global_step': 1886}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  80%|███████▉  | 1761/2212 [09:31<02:17,  3.29batch/s, loss=0.99, lr=2.54e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.990172803401947, 'train/learning_rate': 2.5428571428571332e-05, 'train/global_step': 1896}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  80%|████████  | 1771/2212 [09:34<02:27,  3.00batch/s, loss=1.27, lr=2.55e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.2707505226135254, 'train/learning_rate': 2.5509945750451978e-05, 'train/global_step': 1906}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  81%|████████  | 1781/2212 [09:37<02:06,  3.40batch/s, loss=0.982, lr=2.56e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.9822069406509399, 'train/learning_rate': 2.5591320072332627e-05, 'train/global_step': 1916}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  81%|████████  | 1791/2212 [09:40<02:00,  3.50batch/s, loss=0.954, lr=2.57e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.9536486268043518, 'train/learning_rate': 2.567269439421328e-05, 'train/global_step': 1926}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  81%|████████▏ | 1801/2212 [09:44<02:35,  2.65batch/s, loss=1.04, lr=2.58e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.0443915128707886, 'train/learning_rate': 2.5754068716093926e-05, 'train/global_step': 1936}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  82%|████████▏ | 1811/2212 [09:46<01:55,  3.49batch/s, loss=0.428, lr=2.58e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.4283481538295746, 'train/learning_rate': 2.5835443037974582e-05, 'train/global_step': 1946}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  82%|████████▏ | 1821/2212 [09:50<02:11,  2.96batch/s, loss=0.97, lr=2.59e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.970245361328125, 'train/learning_rate': 2.5916817359855228e-05, 'train/global_step': 1956}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  83%|████████▎ | 1831/2212 [09:52<01:49,  3.48batch/s, loss=1.11, lr=2.6e-5]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.108572244644165, 'train/learning_rate': 2.5998191681735887e-05, 'train/global_step': 1966}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  83%|████████▎ | 1841/2212 [09:56<02:35,  2.39batch/s, loss=0.814, lr=2.61e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.8138083815574646, 'train/learning_rate': 2.607956600361654e-05, 'train/global_step': 1976}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  84%|████████▎ | 1851/2212 [09:59<01:56,  3.10batch/s, loss=0.855, lr=2.62e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.8549678325653076, 'train/learning_rate': 2.6160940325497192e-05, 'train/global_step': 1986}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  84%|████████▍ | 1861/2212 [10:03<02:03,  2.84batch/s, loss=0.949, lr=2.62e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.9486470222473145, 'train/learning_rate': 2.6242314647377838e-05, 'train/global_step': 1996}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  85%|████████▍ | 1871/2212 [10:06<01:42,  3.33batch/s, loss=0.853, lr=2.63e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.853274405002594, 'train/learning_rate': 2.6323688969258488e-05, 'train/global_step': 2006}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  85%|████████▌ | 1881/2212 [10:10<01:43,  3.20batch/s, loss=0.838, lr=2.64e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.8383336663246155, 'train/learning_rate': 2.6405063291139133e-05, 'train/global_step': 2016}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  85%|████████▌ | 1891/2212 [10:13<01:38,  3.27batch/s, loss=1.43, lr=2.65e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.4297350645065308, 'train/learning_rate': 2.6486437613019786e-05, 'train/global_step': 2026}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  86%|████████▌ | 1901/2212 [10:16<01:37,  3.18batch/s, loss=0.848, lr=2.66e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.8480761647224426, 'train/learning_rate': 2.6567811934900432e-05, 'train/global_step': 2036}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  86%|████████▋ | 1911/2212 [10:20<01:55,  2.62batch/s, loss=0.523, lr=2.66e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.5229091644287109, 'train/learning_rate': 2.6649186256781074e-05, 'train/global_step': 2046}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  87%|████████▋ | 1921/2212 [10:23<01:34,  3.10batch/s, loss=0.937, lr=2.67e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.9371833801269531, 'train/learning_rate': 2.6730560578661724e-05, 'train/global_step': 2056}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  87%|████████▋ | 1931/2212 [10:26<01:27,  3.20batch/s, loss=0.861, lr=2.68e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.8608606457710266, 'train/learning_rate': 2.681193490054237e-05, 'train/global_step': 2066}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  88%|████████▊ | 1941/2212 [10:29<01:21,  3.31batch/s, loss=1.21, lr=2.69e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.2059448957443237, 'train/learning_rate': 2.689330922242303e-05, 'train/global_step': 2076}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  88%|████████▊ | 1951/2212 [10:33<01:32,  2.81batch/s, loss=0.816, lr=2.7e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.8161307573318481, 'train/learning_rate': 2.6974683544303685e-05, 'train/global_step': 2086}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  89%|████████▊ | 1961/2212 [10:36<01:21,  3.07batch/s, loss=0.858, lr=2.71e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.8583690524101257, 'train/learning_rate': 2.7056057866184334e-05, 'train/global_step': 2096}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  89%|████████▉ | 1971/2212 [10:39<01:12,  3.33batch/s, loss=0.824, lr=2.71e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.8241732716560364, 'train/learning_rate': 2.713743218806499e-05, 'train/global_step': 2106}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  90%|████████▉ | 1981/2212 [10:43<01:21,  2.85batch/s, loss=1.17, lr=2.72e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.1662240028381348, 'train/learning_rate': 2.7218806509945643e-05, 'train/global_step': 2116}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  90%|█████████ | 1991/2212 [10:47<01:24,  2.63batch/s, loss=0.803, lr=2.73e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.8028278946876526, 'train/learning_rate': 2.73001808318263e-05, 'train/global_step': 2126}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  90%|█████████ | 2001/2212 [10:50<01:20,  2.63batch/s, loss=0.877, lr=2.74e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.8774856328964233, 'train/learning_rate': 2.7381555153706958e-05, 'train/global_step': 2136}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  91%|█████████ | 2011/2212 [10:53<01:08,  2.94batch/s, loss=1.13, lr=2.75e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.1252366304397583, 'train/learning_rate': 2.7462929475587614e-05, 'train/global_step': 2146}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  91%|█████████▏| 2021/2212 [10:57<01:19,  2.41batch/s, loss=0.952, lr=2.75e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.9515444040298462, 'train/learning_rate': 2.754430379746826e-05, 'train/global_step': 2156}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  92%|█████████▏| 2031/2212 [11:00<00:59,  3.02batch/s, loss=1.03, lr=2.76e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.0320278406143188, 'train/learning_rate': 2.7625678119348913e-05, 'train/global_step': 2166}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  92%|█████████▏| 2041/2212 [11:03<00:50,  3.40batch/s, loss=1.25, lr=2.77e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.2548080682754517, 'train/learning_rate': 2.7707052441229572e-05, 'train/global_step': 2176}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  93%|█████████▎| 2051/2212 [11:06<00:50,  3.19batch/s, loss=0.813, lr=2.78e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.8131533861160278, 'train/learning_rate': 2.7788426763110218e-05, 'train/global_step': 2186}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  93%|█████████▎| 2061/2212 [11:09<00:49,  3.03batch/s, loss=0.522, lr=2.79e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.5215580463409424, 'train/learning_rate': 2.7869801084990874e-05, 'train/global_step': 2196}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  94%|█████████▎| 2071/2212 [11:12<00:40,  3.51batch/s, loss=0.844, lr=2.8e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.8438436985015869, 'train/learning_rate': 2.7951175406871523e-05, 'train/global_step': 2206}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  94%|█████████▍| 2081/2212 [11:15<00:36,  3.58batch/s, loss=1.02, lr=2.8e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.0162566900253296, 'train/learning_rate': 2.8032549728752166e-05, 'train/global_step': 2216}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  95%|█████████▍| 2091/2212 [11:18<00:34,  3.47batch/s, loss=0.837, lr=2.81e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.83709317445755, 'train/learning_rate': 2.811392405063282e-05, 'train/global_step': 2226}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  95%|█████████▍| 2101/2212 [11:22<00:37,  2.95batch/s, loss=0.696, lr=2.82e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.6957387328147888, 'train/learning_rate': 2.819529837251347e-05, 'train/global_step': 2236}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  95%|█████████▌| 2111/2212 [11:25<00:38,  2.66batch/s, loss=0.635, lr=2.83e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.6345019340515137, 'train/learning_rate': 2.8276672694394117e-05, 'train/global_step': 2246}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  96%|█████████▌| 2121/2212 [11:28<00:31,  2.93batch/s, loss=0.689, lr=2.84e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.6885377764701843, 'train/learning_rate': 2.8358047016274763e-05, 'train/global_step': 2256}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  96%|█████████▋| 2131/2212 [11:31<00:26,  3.02batch/s, loss=0.886, lr=2.84e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.8864436149597168, 'train/learning_rate': 2.8439421338155412e-05, 'train/global_step': 2266}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  97%|█████████▋| 2141/2212 [11:34<00:20,  3.51batch/s, loss=0.761, lr=2.85e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.7608665227890015, 'train/learning_rate': 2.852079566003606e-05, 'train/global_step': 2276}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  97%|█████████▋| 2151/2212 [11:37<00:19,  3.15batch/s, loss=0.996, lr=2.86e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.9961159229278564, 'train/learning_rate': 2.860216998191672e-05, 'train/global_step': 2286}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  98%|█████████▊| 2161/2212 [11:41<00:14,  3.45batch/s, loss=0.619, lr=2.87e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.6193861365318298, 'train/learning_rate': 2.8683544303797367e-05, 'train/global_step': 2296}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  98%|█████████▊| 2171/2212 [11:44<00:14,  2.92batch/s, loss=0.662, lr=2.88e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.6618478298187256, 'train/learning_rate': 2.8764918625678016e-05, 'train/global_step': 2306}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  99%|█████████▊| 2181/2212 [11:48<00:11,  2.73batch/s, loss=1.02, lr=2.88e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 1.017358660697937, 'train/learning_rate': 2.8846292947558665e-05, 'train/global_step': 2316}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  99%|█████████▉| 2191/2212 [11:51<00:05,  3.50batch/s, loss=0.871, lr=2.89e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.871448814868927, 'train/learning_rate': 2.8927667269439324e-05, 'train/global_step': 2326}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|█████████▉| 2201/2212 [11:54<00:03,  2.97batch/s, loss=0.434, lr=2.9e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.4335782527923584, 'train/learning_rate': 2.9009041591319984e-05, 'train/global_step': 2336}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|█████████▉| 2211/2212 [11:57<00:00,  3.54batch/s, loss=0.91, lr=2.91e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train/batch_loss': 0.9099376797676086, 'train/learning_rate': 2.9090415913200643e-05, 'train/global_step': 2346}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 2212/2212 [11:57<00:00,  3.08batch/s, loss=1.41, lr=2.91e-5]\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "total_loss = 0.0\n",
    "epoch = 1\n",
    "\n",
    "with tqdm(dataloader, unit=\"batch\", desc=f\"Epoch {epoch}\") as progress_bar:\n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            # Move batch to device\n",
    "            batch_encounters = batch['encounter_name']\n",
    "            del batch['encounter_name']\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            batch['ehr'][torch.isinf(batch['ehr'])] = 0\n",
    "            \n",
    "            # Decide on teacher forcing\n",
    "            use_teacher_forcing = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            target_input = batch[\"target\"][:, :-1] if use_teacher_forcing else None\n",
    "\n",
    "            # Forward pass with mixed precision\n",
    "            with torch.cuda.amp.autocast(enabled=trainer_config.mixed_precision):\n",
    "                outputs = model(\n",
    "                    ehr=batch[\"ehr\"],\n",
    "                    prev_cxr=batch[\"prev_cxr\"],\n",
    "                    target_input=None,\n",
    "                    encoder_attention_mask=batch.get(\"attention_mask\"),\n",
    "                    decoder_attention_mask=batch.get(\"attention_mask\"),\n",
    "                    causal_mask=True\n",
    "                )\n",
    "                loss = criterion(outputs, batch[\"target\"])\n",
    "            if torch.isnan(loss):\n",
    "                break\n",
    "\n",
    "            # Backward pass with gradient scaling\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), trainer_config.grad_norm_clip)\n",
    "\n",
    "            # Update weights\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            # Update learning rate\n",
    "            if global_step < warmup_steps:\n",
    "                warmup_scheduler.step()\n",
    "            else:\n",
    "                scheduler.step()\n",
    "\n",
    "            # Update metrics\n",
    "            total_loss += loss.item()\n",
    "            global_step += 1\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix(loss=loss.item(), lr=optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "            # Log metrics\n",
    "            if batch_idx % trainer_config.log_every == 0:\n",
    "                print({\n",
    "                    \"train/batch_loss\": loss.item(),\n",
    "                    \"train/learning_rate\": optimizer.param_groups[0][\"lr\"],\n",
    "                    \"train/global_step\": global_step,\n",
    "                })\n",
    "\n",
    "avg_loss = total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6659b4e-2563-4de8-aa13-0cc34ed47878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 100, 81]), torch.Size([2, 100, 512]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['ehr'].shape, batch['prev_cxr'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "026f7b9e-23fd-4b0f-8859-848cb6ef6175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 593])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([batch['ehr'],  batch['prev_cxr']], dim = -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "936c50bc-4e6b-49e3-b372-aae10d133210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inf found at index: (tensor(3), tensor(64), tensor(46))\n",
      "Inf found at index: (tensor(3), tensor(65), tensor(46))\n",
      "Inf found at index: (tensor(3), tensor(66), tensor(46))\n",
      "Inf found at index: (tensor(3), tensor(67), tensor(46))\n",
      "Inf found at index: (tensor(3), tensor(68), tensor(46))\n",
      "Inf found at index: (tensor(3), tensor(69), tensor(46))\n",
      "Inf found at index: (tensor(3), tensor(70), tensor(46))\n",
      "Inf found at index: (tensor(3), tensor(71), tensor(46))\n",
      "Inf found at index: (tensor(3), tensor(72), tensor(46))\n",
      "Inf found at index: (tensor(3), tensor(73), tensor(46))\n",
      "Inf found at index: (tensor(3), tensor(74), tensor(46))\n",
      "Inf found at index: (tensor(3), tensor(75), tensor(46))\n",
      "Inf found at index: (tensor(3), tensor(76), tensor(46))\n",
      "Inf found at index: (tensor(3), tensor(77), tensor(46))\n",
      "Inf found at index: (tensor(3), tensor(78), tensor(46))\n",
      "Inf found at index: (tensor(3), tensor(79), tensor(46))\n",
      "Inf found at index: (tensor(3), tensor(80), tensor(46))\n",
      "Inf found at index: (tensor(3), tensor(81), tensor(46))\n",
      "Inf found at index: (tensor(3), tensor(82), tensor(46))\n",
      "Inf found at index: (tensor(3), tensor(83), tensor(46))\n",
      "Inf found at index: (tensor(3), tensor(84), tensor(46))\n",
      "Inf found at index: (tensor(3), tensor(85), tensor(46))\n",
      "Inf found at index: (tensor(3), tensor(86), tensor(46))\n",
      "Inf found at index: (tensor(3), tensor(87), tensor(46))\n",
      "Inf found at index: (tensor(3), tensor(88), tensor(46))\n"
     ]
    }
   ],
   "source": [
    "inf_mask = torch.isinf(batch['ehr'])\n",
    "inf_indices = torch.where(inf_mask)\n",
    "num_infs = inf_mask.sum().item()\n",
    "\n",
    "if inf_mask.any():\n",
    "    # For a multi-dimensional tensor\n",
    "    for idx in zip(*torch.where(inf_mask)):\n",
    "        print(f\"Inf found at index: {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d776abe0-ae7c-47b7-8cc1-c33dee274686",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['ehr'][1, :, 45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a8dd9a0-b9b9-46f8-b65a-05c769a55df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(supertable_path / (batch_encounters[1] + '.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "77cf7dbe-282d-42b9-8612-c70b273c17ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, -0.5018181818181818, -0.5018181818181818, -0.505,\n",
       "       -0.505, -0.505, -0.505, -0.505, -0.505, -0.505, -0.25, -0.25,\n",
       "       -0.25, -0.25, -0.5, -0.5, -0.2515625, -0.2515625, -0.2515625,\n",
       "       -0.2515625, -0.2550000000000001, -0.2550000000000001,\n",
       "       -0.2550000000000001, -0.2575, -0.253125, -0.253125, -0.253125,\n",
       "       -0.253125, -0.253125, -0.2625, -0.2625, -0.2625, -0.2625,\n",
       "       -0.2666666666666667, -0.2666666666666667, -0.2666666666666667,\n",
       "       -0.26875, -0.2583333333333334, -0.2583333333333334,\n",
       "       -0.2583333333333334, -0.2583333333333334, -0.2583333333333334,\n",
       "       -0.2625, -0.2625, -0.2625, -0.2666666666666667,\n",
       "       -0.2666666666666667, -0.2666666666666667, -0.275, -0.26875,\n",
       "       -0.26875, -0.26875, -0.26875, -0.2625, -0.2625, -0.2625, -0.2625,\n",
       "       -0.25, -0.25, -0.2583333333333334, -0.2583333333333334, 0.15, 0.15,\n",
       "       0.15, 0.15, -0.2625, -0.2625, -0.26071428571428584,\n",
       "       -0.2583333333333334, -0.2583333333333334, -0.2583333333333334,\n",
       "       -0.2583333333333334, -0.2625, -0.2625, -0.2625, -0.2625, -0.2625,\n",
       "       inf, inf, inf, inf, -0.2583333333333334, -0.2583333333333334,\n",
       "       -0.2583333333333334, -0.2583333333333334, -0.2625, -0.2625,\n",
       "       -0.2625, -0.2625, -0.25, -0.25, -0.278125, -0.25, -0.25, -0.25,\n",
       "       -0.25, -0.25, -0.2625, -0.25, -0.25, -0.25, -0.25, -0.25, -0.25,\n",
       "       -0.25, -0.25, -0.25, -0.25, -0.25, -0.25, -0.25, -0.25, -0.25,\n",
       "       -0.25, -0.25, 0.13333333333333341, 0.15833333333333344,\n",
       "       0.15833333333333344, 0.15833333333333344, 0.15833333333333344,\n",
       "       0.15833333333333344, 0.15833333333333344, 0.15833333333333344,\n",
       "       0.15833333333333344, 0.15833333333333344, 0.15833333333333344,\n",
       "       0.15833333333333344, 0.15833333333333344, 0.15, 0.15, 0.15, 0.15,\n",
       "       0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15,\n",
       "       0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.13333333333333341,\n",
       "       0.15, 0.15, 0.125, 0.125, 0.13333333333333341, 0.13333333333333341,\n",
       "       0.13333333333333341, 0.13333333333333341, 0.13333333333333341,\n",
       "       0.13333333333333341, 0.1, 0.127777777777778, 0.127777777777778,\n",
       "       0.127777777777778, 0.13333333333333341, 0.13333333333333341,\n",
       "       0.13333333333333341, 0.13333333333333341, 0.15, 0.15, 0.15, 0.15,\n",
       "       0.15, 0.15, 0.15, 0.15, 0.15, 0.11666666666666686,\n",
       "       0.11666666666666686, 0.11666666666666686, 0.15, 0.15, 0.15,\n",
       "       0.16666666666666685, 0.16666666666666685, 0.16666666666666685,\n",
       "       0.16666666666666685, 0.16666666666666685, 0.16666666666666685,\n",
       "       0.16666666666666685, 0.16666666666666685, 0.16666666666666685,\n",
       "       0.16666666666666685, 0.16666666666666685, 0.16666666666666685,\n",
       "       0.15833333333333344, 0.15833333333333344, 0.15833333333333344,\n",
       "       0.15833333333333344, 0.15833333333333344, 0.16111111111111115,\n",
       "       0.16111111111111115, 0.16111111111111115, 0.16111111111111115,\n",
       "       0.16111111111111115, 0.16111111111111115, 0.16111111111111115,\n",
       "       0.11666666666666686, 0.12916666666666685, 0.12916666666666685,\n",
       "       0.12916666666666685, 0.12916666666666685, 0.12916666666666685,\n",
       "       0.12916666666666685, 0.13333333333333341, 0.13333333333333341,\n",
       "       0.13333333333333341, 0.125, 0.125, 0.13333333333333341,\n",
       "       0.11666666666666686, 0.15, 0.13333333333333341,\n",
       "       0.13333333333333341, 0.13333333333333341, 0.13333333333333341,\n",
       "       0.13333333333333341, 0.13333333333333341, 0.10833333333333343,\n",
       "       0.10833333333333343, 0.10833333333333343, 0.14166666666666686,\n",
       "       0.14166666666666686, 0.14166666666666686, 0.14166666666666686,\n",
       "       0.14166666666666686, 0.14166666666666686, 0.14166666666666686,\n",
       "       0.14166666666666686, 0.14166666666666686, 0.14166666666666686,\n",
       "       0.14166666666666686, 0.14166666666666686, 0.14166666666666686,\n",
       "       0.14166666666666686, 0.14166666666666686, 0.14166666666666686,\n",
       "       0.14166666666666686, 0.14166666666666686, 0.14166666666666686,\n",
       "       0.14166666666666686, 0.14166666666666686, 0.14166666666666686,\n",
       "       0.14166666666666686, 0.14166666666666686, 0.14166666666666686, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan], dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.values[:, 45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f28f63b5-a4e6-47c0-987f-12456f0b993f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EHR input\n",
      "tensor([[0.9000, 1.8933, 1.2533,  ..., 0.0000, 1.0000, 0.0000],\n",
      "        [1.0100, 1.8933, 1.7400,  ..., 0.0000, 1.0000, 0.0000],\n",
      "        [1.0500, 1.8933, 1.8000,  ..., 0.0000, 1.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.5000, 1.7533, 1.4000,  ..., 0.0000, 1.0000, 0.0000],\n",
      "        [0.5000, 1.7533, 1.4000,  ..., 0.0000, 1.0000, 0.0000],\n",
      "        [0.5000, 1.7533, 1.6000,  ..., 0.0000, 1.0000, 0.0000]])\n",
      "ehr - min: -3.4000000953674316, max: inf, has_nan: False\n",
      "EHR embedding\n",
      "tensor([[ 0.3397, -2.5103, -1.9462,  ..., -0.2305, -1.0521, -0.8516],\n",
      "        [ 0.3448, -2.5688, -1.9900,  ..., -0.2090, -1.0538, -0.8895],\n",
      "        [ 0.3552, -2.6524, -2.0044,  ..., -0.2060, -1.0753, -0.9462],\n",
      "        ...,\n",
      "        [-0.9441, -4.9256, -2.1909,  ...,  2.2478, -3.2096, -0.9323],\n",
      "        [-0.9532, -4.9645, -2.2049,  ...,  2.2638, -3.2335, -0.9412],\n",
      "        [-0.9052, -5.0178, -2.2416,  ...,  2.2919, -3.2731, -0.9600]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "x - min: -inf, max: inf, has_nan: False\n",
      "CXR condition\n",
      "tensor([[ 0.5402, -2.0472, -0.8042,  ...,  0.4987, -0.0979,  1.8794],\n",
      "        [ 0.5402, -2.0472, -0.8042,  ...,  0.4987, -0.0979,  1.8794],\n",
      "        [ 0.5402, -2.0472, -0.8042,  ...,  0.4987, -0.0979,  1.8794],\n",
      "        ...,\n",
      "        [ 0.4632, -2.1577, -0.7180,  ...,  0.1457,  1.5235,  2.7956],\n",
      "        [ 0.4632, -2.1577, -0.7180,  ...,  0.1457,  1.5235,  2.7956],\n",
      "        [ 1.8306, -1.6467,  0.1923,  ...,  0.1267,  1.8964,  2.0057]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "cxr_cond - min: -5.492913246154785, max: 4.810727119445801, has_nan: False\n",
      "EHR embedding with CXR condition\n",
      "tensor([[ 0.8800, -4.5575, -2.7504,  ...,  0.2682, -1.1500,  1.0279],\n",
      "        [ 0.8851, -4.6160, -2.7942,  ...,  0.2897, -1.1518,  0.9899],\n",
      "        [ 0.8954, -4.6996, -2.8085,  ...,  0.2927, -1.1733,  0.9333],\n",
      "        ...,\n",
      "        [-0.4809, -7.0832, -2.9089,  ...,  2.3935, -1.6861,  1.8633],\n",
      "        [-0.4900, -7.1221, -2.9229,  ...,  2.4095, -1.7100,  1.8544],\n",
      "        [ 0.9253, -6.6644, -2.0493,  ...,  2.4187, -1.3767,  1.0457]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "x - min: -inf, max: inf, has_nan: False\n",
      "EHR embedding with CXR condition and positional embedding\n",
      "tensor([[ 0.8800, -3.5575, -2.7504,  ...,  1.2682, -1.1500,  2.0279],\n",
      "        [ 1.7266, -4.0757, -1.9723,  ...,  1.2897, -1.1517,  1.9899],\n",
      "        [ 1.8047, -5.1158, -1.8721,  ...,  1.2927, -1.1731,  1.9333],\n",
      "        ...,\n",
      "        [-0.1013, -8.0084, -3.5343,  ...,  3.3934, -1.6760,  2.8632],\n",
      "        [-1.0633, -7.9414, -2.6379,  ...,  3.4095, -1.6999,  2.8543],\n",
      "        [-0.0739, -6.6246, -1.0991,  ...,  3.4186, -1.3664,  2.0456]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "x - min: -inf, max: inf, has_nan: False\n",
      "EHR embedding with CXR condition and positional embedding after dropout\n",
      "tensor([[ 0.9777, -3.9527, -3.0560,  ...,  1.4091, -1.2778,  2.2532],\n",
      "        [ 1.9184, -0.0000, -2.1915,  ...,  1.4330, -1.2796,  2.2111],\n",
      "        [ 2.0053, -5.6842, -2.0801,  ...,  1.4364, -1.3034,  2.1481],\n",
      "        ...,\n",
      "        [-0.1126, -8.8982, -3.9270,  ...,  3.7705, -1.8623,  3.1814],\n",
      "        [-1.1815, -8.8238, -2.9310,  ...,  3.7883, -1.8887,  3.1715],\n",
      "        [-0.0821, -7.3607, -1.2212,  ...,  3.7985, -1.5182,  2.2729]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "x - min: nan, max: nan, has_nan: True\n",
      "key padding mask\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "Attention mask\n",
      "None\n",
      "x_norm\n",
      "tensor([[ 0.2102, -1.7482, -1.3907,  ...,  0.3820, -0.6856,  0.7166],\n",
      "        [ 0.5373, -0.2138, -1.0703,  ...,  0.3477, -0.7143,  0.6516],\n",
      "        [ 0.5756, -2.3823, -0.9949,  ...,  0.3572, -0.6970,  0.6304],\n",
      "        ...,\n",
      "        [-0.1355, -2.1675, -1.0167,  ...,  0.7629, -0.5402,  0.6261],\n",
      "        [-0.3464, -2.1031, -0.7478,  ...,  0.7962, -0.5090,  0.6538],\n",
      "        [-0.1330, -1.8264, -0.3975,  ...,  0.7701, -0.4672,  0.4147]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "x_norm - min: nan, max: nan, has_nan: True\n",
      "Attention output\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "key padding mask\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "Attention mask\n",
      "None\n",
      "x_norm\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "x_norm - min: nan, max: nan, has_nan: True\n",
      "Attention output\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "key padding mask\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "Attention mask\n",
      "None\n",
      "x_norm\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "x_norm - min: nan, max: nan, has_nan: True\n",
      "Attention output\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "key padding mask\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "Attention mask\n",
      "None\n",
      "x_norm\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "x_norm - min: nan, max: nan, has_nan: True\n",
      "Attention output\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "key padding mask\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "Attention mask\n",
      "None\n",
      "x_norm\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "x_norm - min: nan, max: nan, has_nan: True\n",
      "Attention output\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "key padding mask\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "Attention mask\n",
      "None\n",
      "x_norm\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "x_norm - min: nan, max: nan, has_nan: True\n",
      "Attention output\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Encoder output\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Encoder output after norm\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Encoder output shape:\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder input\n",
      "tensor([[-0.0407, -0.0097,  0.0005,  ..., -0.0048,  0.0413,  0.0113],\n",
      "        [-0.0407, -0.0097,  0.0005,  ..., -0.0048,  0.0413,  0.0113],\n",
      "        [-0.0407, -0.0097,  0.0005,  ..., -0.0048,  0.0413,  0.0113],\n",
      "        ...,\n",
      "        [-0.0407, -0.0097,  0.0005,  ..., -0.0048,  0.0413,  0.0113],\n",
      "        [-0.0407, -0.0097,  0.0005,  ..., -0.0048,  0.0413,  0.0113],\n",
      "        [-0.0407, -0.0097,  0.0005,  ..., -0.0048,  0.0413,  0.0113]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder input after positional embeddings\n",
      "tensor([[-4.0695e-02,  9.9027e-01,  4.9729e-04,  ...,  9.9525e-01,\n",
      "          4.1293e-02,  1.0113e+00],\n",
      "        [ 8.0078e-01,  5.3057e-01,  8.2235e-01,  ...,  9.9525e-01,\n",
      "          4.1397e-02,  1.0113e+00],\n",
      "        [ 8.6860e-01, -4.2588e-01,  9.3691e-01,  ...,  9.9525e-01,\n",
      "          4.1500e-02,  1.0113e+00],\n",
      "        ...,\n",
      "        [ 3.3891e-01, -9.3488e-01, -6.2487e-01,  ...,  9.9519e-01,\n",
      "          5.1348e-02,  1.0112e+00],\n",
      "        [-6.1408e-01, -8.2902e-01,  2.8554e-01,  ...,  9.9519e-01,\n",
      "          5.1452e-02,  1.0112e+00],\n",
      "        [-1.0399e+00,  3.0089e-02,  9.5065e-01,  ...,  9.9519e-01,\n",
      "          5.1556e-02,  1.0112e+00]], grad_fn=<SliceBackward0>)\n",
      "Causal mask\n",
      "tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "        [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
      "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., -inf, -inf],\n",
      "        [0., 0., 0.,  ..., 0., 0., -inf],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Decoder block input\n",
      "tensor([[-1.0807,  0.9794, -0.9984,  ...,  0.9889, -0.9183,  1.0224],\n",
      "        [ 0.5717, -0.0182,  0.6188,  ...,  0.9956, -1.0872,  1.0321],\n",
      "        [ 0.7189, -2.1201,  0.8687,  ...,  0.9961, -1.0964,  1.0328],\n",
      "        ...,\n",
      "        [ 0.0556, -1.9381, -1.4527,  ...,  1.0821, -0.3955,  1.1088],\n",
      "        [-1.4326, -1.7691, -0.0260,  ...,  1.0831, -0.3931,  1.1098],\n",
      "        [-2.0991, -0.4260,  1.0140,  ...,  1.0831, -0.3932,  1.1098]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block self-attention output\n",
      "tensor([[-0.1311, -0.0017,  0.3227,  ...,  0.1713,  0.0252,  0.0624],\n",
      "        [-0.1419, -0.0413,  0.3750,  ...,  0.0881,  0.0154,  0.0805],\n",
      "        [-0.2178, -0.0430,  0.4502,  ...,  0.0435,  0.0437,  0.0459],\n",
      "        ...,\n",
      "        [-0.2534, -0.0768,  0.0439,  ...,  0.1835,  0.3186,  0.2110],\n",
      "        [-0.1972, -0.0736,  0.0323,  ...,  0.1731,  0.3164,  0.2382],\n",
      "        [-0.2432, -0.1045,  0.0573,  ...,  0.1775,  0.2987,  0.2480]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block self-attention output\n",
      "tensor([[-0.1864,  0.9903,  0.3590,  ...,  1.1856,  0.0693,  1.0807],\n",
      "        [ 0.6431,  0.4847,  1.2390,  ...,  1.0931,  0.0585,  1.1007],\n",
      "        [ 0.6266, -0.4737,  1.4371,  ...,  1.0436,  0.0901,  1.0623],\n",
      "        ...,\n",
      "        [ 0.0574, -1.0202, -0.5761,  ...,  1.1990,  0.4053,  1.2457],\n",
      "        [-0.8332, -0.9108,  0.3214,  ...,  1.1876,  0.4030,  1.2759],\n",
      "        [-1.3101, -0.0860,  1.0143,  ...,  1.1925,  0.3834,  1.2868]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block cross-attention input\n",
      "tensor([[-1.1197,  0.7737, -0.2416,  ...,  1.0879, -0.7083,  0.9190],\n",
      "        [ 0.1629, -0.1077,  1.1822,  ...,  0.9327, -0.8372,  0.9455],\n",
      "        [ 0.1528, -1.8495,  1.6281,  ...,  0.9118, -0.8240,  0.9459],\n",
      "        ...,\n",
      "        [-0.3579, -1.9439, -1.2898,  ...,  1.3231,  0.1544,  1.3917],\n",
      "        [-1.6633, -1.7764,  0.0304,  ...,  1.3004,  0.1498,  1.4298],\n",
      "        [-2.3548, -0.5655,  1.0421,  ...,  1.3023,  0.1200,  1.4400]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Decoder block cross-attention output\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block cross-attention output\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block MLP input\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block MLP output\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block input\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block self-attention output\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block self-attention output\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block cross-attention input\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block cross-attention output\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block cross-attention output\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block MLP input\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block MLP output\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block input\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block self-attention output\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block self-attention output\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block cross-attention input\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block cross-attention output\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block cross-attention output\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block MLP input\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block MLP output\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block input\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block self-attention output\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block self-attention output\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block cross-attention input\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block cross-attention output\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block cross-attention output\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block MLP input\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block MLP output\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block input\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block self-attention output\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block self-attention output\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block cross-attention input\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block cross-attention output\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block cross-attention output\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block MLP input\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block MLP output\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block input\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block self-attention output\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block self-attention output\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block cross-attention input\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block cross-attention output\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block cross-attention output\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block MLP input\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder block MLP output\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Decoder output\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n",
      "Output\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "outputs = model(\n",
    "                    ehr=batch[\"ehr\"],\n",
    "                    prev_cxr=batch[\"prev_cxr\"],\n",
    "                    target_input=None,\n",
    "                    encoder_attention_mask=batch.get(\"attention_mask\"),\n",
    "                    decoder_attention_mask=batch.get(\"attention_mask\"),\n",
    "                    causal_mask=True, debug = True, batch_idx_debug = 1\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c4f7b5-4d15-499e-bef6-923681fb59e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(torch)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
